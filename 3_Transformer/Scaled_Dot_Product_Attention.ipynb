{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODUYudWdoc3TjXQR/9iLZL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/3_Transformer/Scaled_Dot_Product_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.6 Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "UFmnUTwKi1dp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KuhDjTiODCnf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.shape[-1])\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill_(mask, -float('inf'))\n",
        "        weights = nn.Softmax(dim=-1)(scores)\n",
        "        attention = torch.matmul(weights, V)\n",
        "        return attention, weights, scores\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 4\n",
        "d_k = 32\n",
        "sdpa = ScaledDotProductAttention()\n",
        "\n",
        "# Randomly initialize Q, K, V matrices\n",
        "Q = torch.randn(seq_len, d_k)\n",
        "K = torch.randn(seq_len, d_k)\n",
        "V = torch.randn(seq_len, d_k)\n",
        "\n",
        "# Call the 'scaled_dot_product_attention' function\n",
        "attention, weights, scores = sdpa(Q, K, V)\n",
        "\n",
        "# Prints output and attention weights.\n",
        "torch.set_printoptions(precision=4, sci_mode=False)\n",
        "print(\"Attention Scores:\\n\", scores)\n",
        "print(\"\\nAttention Weights:\\n\", weights)\n",
        "print(\"\\nAttention:\", attention.size())\n",
        "print(attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-5Pi3HxN4Hu",
        "outputId": "a148d9e7-01a2-4822-a75c-d6a3f761630f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores:\n",
            " tensor([[-2.0405,  0.6809,  0.2939,  1.6505],\n",
            "        [ 0.3717,  0.9928,  2.1701, -0.0975],\n",
            "        [ 0.2492,  0.5965,  0.3470, -0.1436],\n",
            "        [ 0.1514,  2.2306, -0.3693,  0.6462]])\n",
            "\n",
            "Attention Weights:\n",
            " tensor([[0.0150, 0.2282, 0.1550, 0.6018],\n",
            "        [0.1050, 0.1954, 0.6340, 0.0657],\n",
            "        [0.2385, 0.3375, 0.2630, 0.1610],\n",
            "        [0.0890, 0.7120, 0.0529, 0.1460]])\n",
            "\n",
            "Attention: torch.Size([4, 32])\n",
            "tensor([[ 0.2603,  1.1516, -0.1773,  0.9438, -0.9488, -0.7728, -0.8722, -0.9581,\n",
            "         -1.1464, -0.5910,  0.4530,  0.3121,  0.3881, -0.9655, -0.3270, -0.8187,\n",
            "         -0.4375, -0.5775, -0.6903,  1.0332,  0.5667,  0.3945,  0.9742,  0.0393,\n",
            "          0.4837, -0.4920,  0.0027,  0.0325,  0.6299, -0.5945,  0.2930, -0.4423],\n",
            "        [-0.1012,  0.7945, -0.4145, -0.3559,  0.0408, -0.3640, -1.5331, -0.5489,\n",
            "         -0.0262, -0.0160, -0.2952,  0.2813,  0.0801, -1.0530, -0.9572, -0.1233,\n",
            "          0.1993,  0.5157, -0.3686, -1.0353, -0.0099,  0.2089,  0.9106, -0.1853,\n",
            "          1.3476,  0.0209,  0.4063,  0.7783, -0.2069,  0.5702,  0.1315, -0.0572],\n",
            "        [ 0.1796,  0.8732,  0.0381,  0.4346, -0.0266, -0.5708, -1.2519, -0.8209,\n",
            "         -0.5377, -0.5326,  0.1271,  0.0544, -0.3084, -0.8474, -0.7958, -0.5251,\n",
            "          0.0666,  0.1899, -0.4859, -0.2348,  0.1312,  0.6302,  0.1976, -0.4759,\n",
            "          0.6909, -0.1292, -0.1807,  0.4246,  0.1710,  0.0861,  0.1714, -0.0208],\n",
            "        [ 0.2638,  0.8531,  0.4507,  0.4967, -0.0124, -0.5204, -1.5930, -0.8969,\n",
            "         -0.6450, -1.0660,  0.7650,  0.3625, -1.2322, -0.9599, -0.4465, -0.4294,\n",
            "         -0.1366, -0.0848, -0.7511, -0.2977,  0.3995,  0.9391,  0.2428, -0.1029,\n",
            "          0.6879, -0.0554, -0.8345,  0.1146,  0.3328,  0.0640, -0.0624,  0.0133]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "attention, weights, scores = sdpa(Q, K, V, mask)\n",
        "\n",
        "print(\"Mask:\\n\", mask)\n",
        "print(\"\\nAttention Scores:\\n\", scores)\n",
        "print(\"\\nAttention Weights:\\n\", weights)\n",
        "print(\"\\nAttention:\", attention.size())\n",
        "print(attention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypVT6FjSgR58",
        "outputId": "eb02576f-7007-413b-aa33-70caa6888e34"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask:\n",
            " tensor([[False,  True,  True,  True],\n",
            "        [False, False,  True,  True],\n",
            "        [False, False, False,  True],\n",
            "        [False, False, False, False]])\n",
            "\n",
            "Attention Scores:\n",
            " tensor([[-2.0405,    -inf,    -inf,    -inf],\n",
            "        [ 0.3717,  0.9928,    -inf,    -inf],\n",
            "        [ 0.2492,  0.5965,  0.3470,    -inf],\n",
            "        [ 0.1514,  2.2306, -0.3693,  0.6462]])\n",
            "\n",
            "Attention Weights:\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3495, 0.6505, 0.0000, 0.0000],\n",
            "        [0.2842, 0.4023, 0.3135, 0.0000],\n",
            "        [0.0890, 0.7120, 0.0529, 0.1460]])\n",
            "\n",
            "Attention: torch.Size([4, 32])\n",
            "tensor([[     0.5250,      0.8303,      0.3605,      1.4981,      0.6162,\n",
            "             -0.9184,     -0.3429,     -1.1141,     -0.8505,     -0.5119,\n",
            "             -0.5255,     -1.2378,      0.0712,     -0.1013,     -1.5522,\n",
            "             -1.2822,      0.6295,      0.7240,     -0.0341,      0.4258,\n",
            "             -0.4804,      1.1067,     -2.0425,     -2.2513,     -0.4764,\n",
            "             -0.2635,     -0.2840,      0.6405,      0.2339,     -0.2724,\n",
            "              0.6006,      0.3582],\n",
            "        [     0.3434,      0.7770,      0.5982,      0.6781,      0.3781,\n",
            "             -0.5701,     -1.4333,     -0.9488,     -0.5952,     -1.0577,\n",
            "              0.5213,     -0.0506,     -1.3043,     -0.7271,     -0.7576,\n",
            "             -0.5715,      0.1316,      0.2341,     -0.5837,     -0.4048,\n",
            "              0.1347,      1.1321,     -0.5587,     -0.7084,      0.4097,\n",
            "             -0.0084,     -0.9185,      0.2582,      0.2487,      0.1192,\n",
            "              0.0247,      0.2161],\n",
            "        [     0.1334,      0.7685,      0.1120,      0.1891,      0.2951,\n",
            "             -0.4758,     -1.4473,     -0.7554,     -0.2915,     -0.5205,\n",
            "              0.0453,      0.0242,     -0.6094,     -0.8406,     -0.9322,\n",
            "             -0.3743,      0.2284,      0.4526,     -0.4344,     -0.7531,\n",
            "             -0.0009,      0.7025,      0.0105,     -0.5800,      0.8270,\n",
            "              0.0175,     -0.2572,      0.5586,     -0.0048,      0.3635,\n",
            "              0.0999,      0.1226],\n",
            "        [     0.2638,      0.8531,      0.4507,      0.4967,     -0.0124,\n",
            "             -0.5204,     -1.5930,     -0.8969,     -0.6450,     -1.0660,\n",
            "              0.7650,      0.3625,     -1.2322,     -0.9599,     -0.4465,\n",
            "             -0.4294,     -0.1366,     -0.0848,     -0.7511,     -0.2977,\n",
            "              0.3995,      0.9391,      0.2428,     -0.1029,      0.6879,\n",
            "             -0.0554,     -0.8345,      0.1146,      0.3328,      0.0640,\n",
            "             -0.0624,      0.0133]])\n"
          ]
        }
      ]
    }
  ]
}