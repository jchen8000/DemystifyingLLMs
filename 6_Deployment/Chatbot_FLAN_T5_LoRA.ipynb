{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQU9iDKrDWeQkot2T1KXTb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "state": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/Chatbot_FLAN_T5_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 5.11 Chatbot, Example of LLM-Powered Application\n",
        "\n",
        "**Chatbot built with local LLM, flan-t5-base plus LoRA.**"
      ],
      "metadata": {
        "id": "wc1wmPFS2tfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure flan-t5-base_lora model is in the output folder"
      ],
      "metadata": {
        "id": "f0m2YXNX2zsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install peft --quiet\n",
        "!pip install sentencepiece --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDi0rf5XC2V-",
        "outputId": "8cf3b40c-c241-472e-8cc5-e8ee15ca92fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/199.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/297.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import GenerationConfig\n",
        "from peft import PeftModel, PeftConfig"
      ],
      "metadata": {
        "id": "2QP9787zD3SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FLAN-T5-Base model and tokenizer\n",
        "base_model_name = 'google/flan-t5-base'\n",
        "lora_model_name = '../5_Fine-Tuning/outputs/flan-t5-base_lora'\n",
        "\n",
        "def load_model(base_model_name, lora_model_name):\n",
        "    tokenizer = T5Tokenizer.from_pretrained(base_model_name)\n",
        "    base_model = T5ForConditionalGeneration.from_pretrained(base_model_name,\n",
        "                                                       torch_dtype=torch.bfloat16)\n",
        "    model = PeftModel.from_pretrained(base_model,\n",
        "                                      lora_model_name,\n",
        "                                      torch_dtype=torch.bfloat16,\n",
        "                                      is_trainnable=False)\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "def generate_output(tokenizer, model, input_text, max_length=200):\n",
        "    # Tokenize the input text and generate the model's output\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1) )\n",
        "\n",
        "    # Decode the generated tokens to a string\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "tokenizer, model = load_model(base_model_name, lora_model_name)\n",
        "\n",
        "\n",
        "total_parameter = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Parameters of the model: {total_parameter:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "12da5f7cfc5a4529b5fc2f590f75c069",
            "99cf2956ec544d2bb2402d5d96df7864",
            "db36535ee904429e9d66eb34b877e7f6",
            "dc0cad5604c74aef9b7d12aa680eab07",
            "dd6a20add16647488ba390ed29a29271",
            "2a671bb8a4314c7c9a910c4e2d46f8c4",
            "857e17dd43d94a3c8ecb2c13a3755d0a",
            "cc3ee583bfc7458a87dc202e822986f8",
            "525e34bcaffa4941ac80c87b763ad1e7",
            "74d4f85f22944db5af8ac18c44f822ec",
            "deda93c7dc4b4099adc64b43ffb73dfe",
            "722c66f99b5c4c1283f96471f0f18d27",
            "87b2b7073a184ca98650d661a24b48e9",
            "accbb3da20fd49aab93e54927878ad0c",
            "87d91a3f46184175a143971716ebc9f2",
            "656556ac5f1c4d73b6413b3066255182",
            "1d00222823a949d3987d2d281cfc28a6",
            "5118a4b8273943c7a101913dd3942843",
            "bcecf236fbff4765b31a584915ac59b1",
            "248f8ff47edb45f39d6ab2b65e0b239c",
            "b81e552b2dcd4fc1a671fe076197de6e",
            "99f0a7a0e2974bc29ea990e92ceb8c39",
            "9597ffbd66e44481b4c34c45de0c78b8",
            "cdee1d65914c4f13927a8898801d6b1c",
            "10d3b3ca93034e2cb9232386f7867664",
            "7189d4b0b73b471486b2352a2020bbed",
            "d782a9fbae6c485a899ee4f66f212a32",
            "e1ab8856fe954743a516b01515757110",
            "cb32156eefd14124bd08571e310c3080",
            "5b9bd52ba51b4813b36dc71c2fcac2e0",
            "f128f67dbe384f3f88cb8d91ebc48999",
            "f5bdb49bfa4b499e9630b4965caa43ca",
            "ec70fee0584e4983a24fe97dbd18f7eb",
            "f0c6f742b11c454f808cd3b8299ae055",
            "75cfb698fc5b479ea95a56e7ac458f4f",
            "f4dc5020c9b948f5b49c9915f2b51b2d",
            "8ac603f6ca29440f8be15e624e3c05be",
            "bf337cefeee34b698210457b016bcd4c",
            "d0f12f6aa59e40a29ffa7030dd9ac262",
            "3bddf3343ddc4fc593202d55942b6005",
            "051762b0be3145a2ae26b6b22aa86868",
            "bbb974d47cd443258404b2c887bacf18",
            "685f3a2cc3b6458dbfbfefacfe232fde",
            "6208ba07368e48e083eb4a5a40adb0fe",
            "f3151688168e4087bcff555d4db96883",
            "8f4eeaf9f5184bd1a2914bc08dd4ad5a",
            "d7d0e4b300fb4387b6396bb7ae41a737",
            "aeb455d5f20543cd9c12a4a7843abb83",
            "44232747a4f44173843c43b32191f8d7",
            "50471eedd4754a54b2d3525359a1fdca",
            "4a5f88e797b04b38bcca9528935436b1",
            "d3a996cfce4f45bdabd5e30c93622f7d",
            "ec4b91c727564e6f8b4943c067ad1ba1",
            "66bcd8168e424436839e750fdc8b67cd",
            "062f04b7b742473e83448dae7f4c6bab",
            "fb80a2e671514a919b960e4362fff585",
            "1ba8353633cb4ff4bf445f4352782f6f",
            "e8eaf39375dd4a858d1f839563ec1bc1",
            "8d3ea9b5adc54300b5b2a40b699b461f",
            "22de3b4c9b634998a7a1868243edc470",
            "cc3a5c27e4d348e29193ce94380ba526",
            "1605ea87b99849d38d08d62a40c5a768",
            "a064f912719c440ca527569c74409786",
            "748478ee52a048079d4dc27a74442823",
            "c6c199efdf8c4a53907f397236d05bb8",
            "c103af80205e4c969c87b8814b701522",
            "d973f7a572cd4549a9ee525fd3e9b4b7",
            "91f9f1a5884e45f49db012af8d0a23f4",
            "97ce673e45cc4ab18f79d9706486d116",
            "88328ed57c1448d48331d12e7b27559e",
            "3bbd2d97211e41df821728c084578eb2",
            "868b07593f1e41a393789ffab2ca9bf8",
            "f86abd8a290649e1b4884789f3bc1b07",
            "a56d8b643395413185e7fb04e9c65891",
            "9984c5affe68472587aa1cca0ec9547a",
            "f91f46b83ee4462481c0a21192141606",
            "74236509956548fbaca0ca0f59200dba"
          ]
        },
        "id": "MxrMp5UEDVBB",
        "outputId": "f2a00d6f-90e8-4bbe-ea74-b8b676f1b18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12da5f7cfc5a4529b5fc2f590f75c069"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "722c66f99b5c4c1283f96471f0f18d27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9597ffbd66e44481b4c34c45de0c78b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0c6f742b11c454f808cd3b8299ae055"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3151688168e4087bcff555d4db96883"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb80a2e671514a919b960e4362fff585"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d973f7a572cd4549a9ee525fd3e9b4b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters of the model: 249,347,328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oZgb7feCrRQ",
        "outputId": "91d13a15-67be-413f-a82a-662365fa9f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot initialized. You can start chatting now (type 'quit' to stop)!\n",
            "\n",
            "You: How are you doing today?\n",
            "Chatbot: You are doing well today.\n",
            "\n",
            "You: Summarize:  saludos,es que tengo un problema con mi iPhone,y lo que ocurre es que no puedo llamar ni recibir llamadas,y no s qu ms hacer,ya que acud a Apple Support y aunque intenten llamarme,no entran llamadas,quiero saber cual es su recomendacin.Gracias. <BR> We offer support via Twitter in English. Get help in Spanish here <LINK> or join <LINK> <BR> AppleSupport I can speak English so so,can you help me?i have an iPhone 7,and 1 week ago,I cant make calls or receive them,what I do? <BR> Being able to make and receive calls is important and wed like to help. Which exact iOS version is installed on it and have you tried any steps yet? <BR> AppleSupport I have iOS 11.1.2 and I have tried everything what the page said.im Colombian. <BR> Is this the page youve tried all the steps from <LINK> yes, what did you find out when you contacted your carrier and when you restored it as new in iTunes? <BR> AppleSupport My Carrier didnt give me a solution,and I dont want restored my phone,that scary me.,can you do something or. I dont know if is the iOS. <BR> To make sure, did your carrier find everything ok on their end when you asked them about the section Contact your carrier in the link we provided? <BR> AppleSupport Yes,I contact them but they dont give me a solution,and yes,I complete everything you say me,without restored. I dont like it. <BR> Got it. Let us know if you have a recent backup of your data in DM. Well continue there <LINK> <BR> AppleSupport Not new but recent.my information can delete? Or my pictures? Or my messages? <BR> With restoring, your information will be deleted, which is why you want to have a recent backup. You can follow the steps in this link for restoring <LINK> If you have any questions, send them over to us in DM from this link <LINK> <BR> AppleSupport Ok thanks,Ill ask you question,but,the best idea can be than I wait for a new iOS,thats can be the problem? <BR> Restoring your device will place a new version of the iOS on your device which can help resolve this issue. Lets discuss it more in DM. <LINK> <BR> AppleSupport Ok,thank you very much. <BR>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Advice: I have a problem with my iPhone and I can't make calls or receive them,what I do?\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def chatbot():\n",
        "\n",
        "    print(\"Chatbot initialized. You can start chatting now (type 'quit' to stop)!\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Check if the user wants to quit\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        response = generate_output(tokenizer, model, user_input)\n",
        "\n",
        "        # Print the generated text\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ]
    }
  ]
}