{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/Chatbot_FLAN_T5_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc1wmPFS2tfq"
      },
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 6.11 Chatbot, Example of LLM-Powered Application\n",
        "\n",
        "### **Build a Chatbot built with Local LLM plus a LoRA Model**\n",
        "\n",
        "This notebook demonstrates how to load and run a local LLM entirely on your machine (or a Google Colab GPU runtime), and enhance its capabilities using a LoRA fine‑tuned adapter. The LoRA adapter used here was created earlier in **Section 5.3**, where we fine‑tuned ```google/flan-t5-base``` on the TweetSum summarization dataset.\n",
        "\n",
        "To ensure reproducibility, the notebook downloads the fine‑tuned LoRA checkpoints directly from our GitHub repository. \n",
        "\n",
        "---\n",
        "\n",
        "### **Key Objectives**\n",
        "\n",
        "- **Local Model Execution**  \n",
        "  Load the ```google/flan-t5-base``` model directly from Hugging Face and run it locally on GPU using PyTorch.\n",
        "\n",
        "- **Apply a Fine‑Tuned LoRA Adapter**  \n",
        "  Attach the fine‑tuned LoRA weights (created in Section 5.3) to the base model using the ```PEFT``` library.\n",
        "\n",
        "- **Text Generation Pipeline**  \n",
        "  Generate responses—such as Tweet‑style summaries or general conversational output—fully offline without relying on external inference APIs.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ **Hardware & Compatibility Requirements**\n",
        "\n",
        "This notebook requires a GPU runtime (such as the Google Colab T4 GPU) because the model is executed locally using PyTorch with CUDA acceleration. Running on CPU is possible but not recommended due to performance limitations.\n",
        "\n",
        "This notebook is confirmed working as of **January 2026**. \n",
        "\n",
        "While the workflow reflects current best practices, deep‑learning libraries evolve rapidly, and future updates to Transformers, PEFT, or CUDA tooling may require adjustments to this notebook.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDi0rf5XC2V-",
        "outputId": "8cf3b40c-c241-472e-8cc5-e8ee15ca92fb"
      },
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "    torch==2.9.0+cu126 \\\n",
        "    torchvision==0.24.0+cu126 \\\n",
        "    torchaudio==2.9.0+cu126 \\\n",
        "    transformers==4.57.6 \\\n",
        "    peft==0.18.1 \\\n",
        "    sentencepiece==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QP9787zD3SX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import T5Tokenizer\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import GenerationConfig\n",
        "from peft import PeftModel, PeftConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download the LoRA model from github repo.  \n",
        "\n",
        "In Section 5.3, we fine‑tuned ```google/flan-t5-base``` model on the TweetSum summarization dataset, and we created the LoRA model called ```flan-t5-base_lora```. Now we download the LoRA model from github repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf lora_model\n",
        "\n",
        "!mkdir -p lora_model\n",
        "!git clone --depth 1 --filter=blob:none --sparse \\\n",
        "    https://github.com/jchen8000/DemystifyingLLMs.git temp_repo\n",
        "\n",
        "%cd temp_repo\n",
        "!git sparse-checkout set \"5_Fine-Tuning/outputs/flan-t5-base_lora\"\n",
        "%cd ..\n",
        "\n",
        "!mv temp_repo/5_Fine-Tuning/outputs/flan-t5-base_lora lora_model/\n",
        "!rm -rf temp_repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load the Tokenizer, Base Model, and LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "12da5f7cfc5a4529b5fc2f590f75c069",
            "99cf2956ec544d2bb2402d5d96df7864",
            "db36535ee904429e9d66eb34b877e7f6",
            "dc0cad5604c74aef9b7d12aa680eab07",
            "dd6a20add16647488ba390ed29a29271",
            "2a671bb8a4314c7c9a910c4e2d46f8c4",
            "857e17dd43d94a3c8ecb2c13a3755d0a",
            "cc3ee583bfc7458a87dc202e822986f8",
            "525e34bcaffa4941ac80c87b763ad1e7",
            "74d4f85f22944db5af8ac18c44f822ec",
            "deda93c7dc4b4099adc64b43ffb73dfe",
            "722c66f99b5c4c1283f96471f0f18d27",
            "87b2b7073a184ca98650d661a24b48e9",
            "accbb3da20fd49aab93e54927878ad0c",
            "87d91a3f46184175a143971716ebc9f2",
            "656556ac5f1c4d73b6413b3066255182",
            "1d00222823a949d3987d2d281cfc28a6",
            "5118a4b8273943c7a101913dd3942843",
            "bcecf236fbff4765b31a584915ac59b1",
            "248f8ff47edb45f39d6ab2b65e0b239c",
            "b81e552b2dcd4fc1a671fe076197de6e",
            "99f0a7a0e2974bc29ea990e92ceb8c39",
            "9597ffbd66e44481b4c34c45de0c78b8",
            "cdee1d65914c4f13927a8898801d6b1c",
            "10d3b3ca93034e2cb9232386f7867664",
            "7189d4b0b73b471486b2352a2020bbed",
            "d782a9fbae6c485a899ee4f66f212a32",
            "e1ab8856fe954743a516b01515757110",
            "cb32156eefd14124bd08571e310c3080",
            "5b9bd52ba51b4813b36dc71c2fcac2e0",
            "f128f67dbe384f3f88cb8d91ebc48999",
            "f5bdb49bfa4b499e9630b4965caa43ca",
            "ec70fee0584e4983a24fe97dbd18f7eb",
            "f0c6f742b11c454f808cd3b8299ae055",
            "75cfb698fc5b479ea95a56e7ac458f4f",
            "f4dc5020c9b948f5b49c9915f2b51b2d",
            "8ac603f6ca29440f8be15e624e3c05be",
            "bf337cefeee34b698210457b016bcd4c",
            "d0f12f6aa59e40a29ffa7030dd9ac262",
            "3bddf3343ddc4fc593202d55942b6005",
            "051762b0be3145a2ae26b6b22aa86868",
            "bbb974d47cd443258404b2c887bacf18",
            "685f3a2cc3b6458dbfbfefacfe232fde",
            "6208ba07368e48e083eb4a5a40adb0fe",
            "f3151688168e4087bcff555d4db96883",
            "8f4eeaf9f5184bd1a2914bc08dd4ad5a",
            "d7d0e4b300fb4387b6396bb7ae41a737",
            "aeb455d5f20543cd9c12a4a7843abb83",
            "44232747a4f44173843c43b32191f8d7",
            "50471eedd4754a54b2d3525359a1fdca",
            "4a5f88e797b04b38bcca9528935436b1",
            "d3a996cfce4f45bdabd5e30c93622f7d",
            "ec4b91c727564e6f8b4943c067ad1ba1",
            "66bcd8168e424436839e750fdc8b67cd",
            "062f04b7b742473e83448dae7f4c6bab",
            "fb80a2e671514a919b960e4362fff585",
            "1ba8353633cb4ff4bf445f4352782f6f",
            "e8eaf39375dd4a858d1f839563ec1bc1",
            "8d3ea9b5adc54300b5b2a40b699b461f",
            "22de3b4c9b634998a7a1868243edc470",
            "cc3a5c27e4d348e29193ce94380ba526",
            "1605ea87b99849d38d08d62a40c5a768",
            "a064f912719c440ca527569c74409786",
            "748478ee52a048079d4dc27a74442823",
            "c6c199efdf8c4a53907f397236d05bb8",
            "c103af80205e4c969c87b8814b701522",
            "d973f7a572cd4549a9ee525fd3e9b4b7",
            "91f9f1a5884e45f49db012af8d0a23f4",
            "97ce673e45cc4ab18f79d9706486d116",
            "88328ed57c1448d48331d12e7b27559e",
            "3bbd2d97211e41df821728c084578eb2",
            "868b07593f1e41a393789ffab2ca9bf8",
            "f86abd8a290649e1b4884789f3bc1b07",
            "a56d8b643395413185e7fb04e9c65891",
            "9984c5affe68472587aa1cca0ec9547a",
            "f91f46b83ee4462481c0a21192141606",
            "74236509956548fbaca0ca0f59200dba"
          ]
        },
        "id": "MxrMp5UEDVBB",
        "outputId": "f2a00d6f-90e8-4bbe-ea74-b8b676f1b18d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12da5f7cfc5a4529b5fc2f590f75c069",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "722c66f99b5c4c1283f96471f0f18d27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9597ffbd66e44481b4c34c45de0c78b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0c6f742b11c454f808cd3b8299ae055",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3151688168e4087bcff555d4db96883",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb80a2e671514a919b960e4362fff585",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d973f7a572cd4549a9ee525fd3e9b4b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters of the model: 249,347,328\n"
          ]
        }
      ],
      "source": [
        "base_model_name = 'google/flan-t5-base'\n",
        "lora_model_name = './lora_model/flan-t5-base_lora'\n",
        "\n",
        "def load_model(base_model_name, lora_model_name):\n",
        "\n",
        "    # Loading the base model and tokenizer into memory\n",
        "    tokenizer = T5Tokenizer.from_pretrained(base_model_name, legacy=True)\n",
        "    base_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        base_model_name,\n",
        "        dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Applying LoRA adapter on top of the local base model\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        lora_model_name,\n",
        "        dtype=torch.bfloat16,\n",
        "        is_trainnable=False\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "tokenizer, model = load_model(base_model_name, lora_model_name)\n",
        "\n",
        "total_parameter = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Parameters of the model: {total_parameter:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a Helper for Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_output(tokenizer, model, input_text, max_length=200):\n",
        "    # Tokenize the input text and generate the model's output\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1) )\n",
        "\n",
        "    # Decode the generated tokens to a string\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define and Run the Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oZgb7feCrRQ",
        "outputId": "91d13a15-67be-413f-a82a-662365fa9f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot initialized. You can start chatting now (type 'quit' to stop)!\n",
            "\n",
            "You: How are you doing today?\n",
            "Chatbot: You are doing well today.\n",
            "\n",
            "You: Summarize:  saludos,es que tengo un problema con mi iPhone,y lo que ocurre es que no puedo llamar ni recibir llamadas,y no s qu ms hacer,ya que acud a Apple Support y aunque intenten llamarme,no entran llamadas,quiero saber cual es su recomendacin.Gracias. <BR> We offer support via Twitter in English. Get help in Spanish here <LINK> or join <LINK> <BR> AppleSupport I can speak English so so,can you help me?i have an iPhone 7,and 1 week ago,I cant make calls or receive them,what I do? <BR> Being able to make and receive calls is important and wed like to help. Which exact iOS version is installed on it and have you tried any steps yet? <BR> AppleSupport I have iOS 11.1.2 and I have tried everything what the page said.im Colombian. <BR> Is this the page youve tried all the steps from <LINK> yes, what did you find out when you contacted your carrier and when you restored it as new in iTunes? <BR> AppleSupport My Carrier didnt give me a solution,and I dont want restored my phone,that scary me.,can you do something or. I dont know if is the iOS. <BR> To make sure, did your carrier find everything ok on their end when you asked them about the section Contact your carrier in the link we provided? <BR> AppleSupport Yes,I contact them but they dont give me a solution,and yes,I complete everything you say me,without restored. I dont like it. <BR> Got it. Let us know if you have a recent backup of your data in DM. Well continue there <LINK> <BR> AppleSupport Not new but recent.my information can delete? Or my pictures? Or my messages? <BR> With restoring, your information will be deleted, which is why you want to have a recent backup. You can follow the steps in this link for restoring <LINK> If you have any questions, send them over to us in DM from this link <LINK> <BR> AppleSupport Ok thanks,Ill ask you question,but,the best idea can be than I wait for a new iOS,thats can be the problem? <BR> Restoring your device will place a new version of the iOS on your device which can help resolve this issue. Lets discuss it more in DM. <LINK> <BR> AppleSupport Ok,thank you very much. <BR>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (641 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Advice: I have a problem with my iPhone and I can't make calls or receive them,what I do?\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def chatbot():\n",
        "\n",
        "    print(\"Chatbot initialized. You can start chatting now (type 'quit' to stop)!\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Check if the user wants to quit\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        response = generate_output(tokenizer, model, user_input)\n",
        "\n",
        "        # Print the generated text\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMQU9iDKrDWeQkot2T1KXTb",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "state": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
