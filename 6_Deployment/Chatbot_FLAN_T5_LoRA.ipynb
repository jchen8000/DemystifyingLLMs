{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/Chatbot_FLAN_T5_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc1wmPFS2tfq"
      },
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 6.11 Chatbot, Example of LLM-Powered Application\n",
        "\n",
        "### **Build a Chatbot with Local LLM plus a LoRA Adapter**\n",
        "\n",
        "This notebook demonstrates how to load and run a local LLM entirely on your machine (or a Google Colab GPU runtime), and enhance its capabilities using a LoRA fine‑tuned adapter. The LoRA adapter used here was created earlier in **Section 5.3**, where we fine‑tuned ```google/flan-t5-base``` on the TweetSum summarization dataset.\n",
        "\n",
        "To ensure reproducibility, the notebook downloads the fine‑tuned LoRA checkpoints directly from our GitHub repository.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Objectives**\n",
        "\n",
        "- **Local Model Execution**  \n",
        "  Load the ```google/flan-t5-base``` model directly from Hugging Face and run it locally on GPU using PyTorch.\n",
        "\n",
        "- **Apply a Fine‑Tuned LoRA Adapter**  \n",
        "  Attach the fine‑tuned LoRA weights (created in Section 5.3) to the base model using the ```PEFT``` library.\n",
        "\n",
        "- **Text Generation Pipeline**  \n",
        "  Generate responses—such as Tweet‑style summaries or general conversational output—fully offline without relying on external inference APIs.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ **Hardware & Compatibility Requirements**\n",
        "\n",
        "This notebook requires a GPU runtime (such as the Google Colab T4 GPU) because the model is executed locally using PyTorch with CUDA acceleration. Running on CPU is possible but not recommended due to performance limitations.\n",
        "\n",
        "This notebook is confirmed working as of **January 2026**.\n",
        "\n",
        "While the workflow reflects current best practices, deep‑learning libraries evolve rapidly, and future updates to Transformers, PEFT, or CUDA tooling may require adjustments to this notebook.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SDi0rf5XC2V-"
      },
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "    torch==2.9.0+cu126 \\\n",
        "    torchvision==0.24.0+cu126 \\\n",
        "    torchaudio==2.9.0+cu126 \\\n",
        "    transformers==4.57.6 \\\n",
        "    peft==0.18.1 \\\n",
        "    sentencepiece==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2QP9787zD3SX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import T5Tokenizer\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import GenerationConfig\n",
        "from peft import PeftModel, PeftConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f69CP3hVP-d8"
      },
      "source": [
        "#### Download the LoRA model from github repo.  \n",
        "\n",
        "In Section 5.3, we fine‑tuned ```google/flan-t5-base``` model on the TweetSum summarization dataset, and we created the LoRA model called ```flan-t5-base_lora```. Now we download the LoRA model from github repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "woLmAhc8P-d9",
        "outputId": "80ae8c69-30c2-41b9-f27c-fd61ae923a1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 14 (delta 0), reused 9 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14/14), done.\n",
            "remote: Enumerating objects: 2, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 2 (delta 0), reused 1 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (2/2), 2.22 KiB | 2.22 MiB/s, done.\n",
            "/content/temp_repo\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (4/4), 2.66 MiB | 11.46 MiB/s, done.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!rm -rf lora_model\n",
        "\n",
        "!mkdir -p lora_model\n",
        "!git clone --depth 1 --filter=blob:none --sparse \\\n",
        "    https://github.com/jchen8000/DemystifyingLLMs.git temp_repo\n",
        "\n",
        "%cd temp_repo\n",
        "!git sparse-checkout set \"5_Fine-Tuning/outputs/flan-t5-base_lora\"\n",
        "%cd ..\n",
        "\n",
        "!mv temp_repo/5_Fine-Tuning/outputs/flan-t5-base_lora lora_model/\n",
        "!rm -rf temp_repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_9_AkdgP-d9"
      },
      "source": [
        "#### Load the Tokenizer, Base Model, and LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxrMp5UEDVBB",
        "outputId": "7c167afd-81b4-4394-8116-883c926dbd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters of the model: 249,347,328\n"
          ]
        }
      ],
      "source": [
        "base_model_name = 'google/flan-t5-base'\n",
        "lora_model_name = './lora_model/flan-t5-base_lora'\n",
        "\n",
        "def load_model(base_model_name, lora_model_name):\n",
        "\n",
        "    # Loading the base model and tokenizer into memory\n",
        "    tokenizer = T5Tokenizer.from_pretrained(base_model_name, legacy=True)\n",
        "    base_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        base_model_name,\n",
        "        dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Applying LoRA adapter on top of the local base model\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        lora_model_name,\n",
        "        dtype=torch.bfloat16,\n",
        "        is_trainnable=False\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "tokenizer, model = load_model(base_model_name, lora_model_name)\n",
        "\n",
        "total_parameter = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Parameters of the model: {total_parameter:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mluYm8yP-d-"
      },
      "source": [
        "#### Define a Helper for Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IGz1lLhuP-d-"
      },
      "outputs": [],
      "source": [
        "def generate_output(tokenizer, model, input_text, max_length=200):\n",
        "    # Tokenize the input text and generate the model's output\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1) )\n",
        "\n",
        "    # Decode the generated tokens to a string\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdRkNXdJP-d_"
      },
      "source": [
        "#### Define and Run the Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oZgb7feCrRQ",
        "outputId": "b655366b-331a-4047-d3f2-8343f3255743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: All set!\n",
            "I'm a text‑summarizing chatbot trained on the TweetSum dataset.\n",
            "I'm ready to summarize any text you provide.\n",
            "(Type 'quit' to stop)\n",
            "\n",
            "You: Summarize the text: AmazonHelp ok, I need to change the shipping address on some shipments, I just realized theyre going to the wrong place. <BR> If the order hasnt entered the shipping process then you can edit the <LINK> KM <BR> AmazonHelp I successfully did it with 2 items, but someone named murfbooks is being difficult. I told them I wanted a refund if they wont cooperate. <BR> Hi, glad to hear you were able to do this for both items. Have they submitted the refund ? CR <BR> AmazonHelp No. I only put in the initial order an hour ago. They were quick to respond the first time, but now have not replied for about 15 minutes. <BR> Ah okay Has the order been marked as shipped? NV <BR> AmazonHelp I am not sure. It says edit order, but it wont let me cuz its 3rd party. <LINK> <BR> Please allow the seller a bit longer to respond. AT <BR>\n",
            "Chatbot: Customer is complaining about the shipping address on some shipments, he just realized they are going to the wrong place.\n",
            "\n",
            "\n",
            "You: Summarize the text: AskAmex Where do I write to address a customer service issue to higher management? <BR> Hi Chris. Which U.S. based card is this concerning? Please do not release any personal or card information. Clarissa <BR> Hi I never heard back from you. Let me know if I can assist. All the best. Clarissa <BR> AskAmex It was the Delta sky miles card. <BR> Good Morning. What is this concerning and I may be able to help? RK <BR> AskAmex Signed up for new card with Delta to book immediately book tix. Card number didnt come up. Customer svce refused to help. <BR> Good morning, thanks for reaching out. Please call our New Accounts Team at 8773993086, for assistance. Theyre available, <BR> Monday Friday 800 AM to 1200 AM ET Saturday 1000 AM to 600 PM ET. Sorry for the inconvenience.In <BR>\n",
            "Chatbot: Customer svce refused to help. Agent is available Monday Friday from 1200 AM to 1200 AM ET on Saturday.\n",
            "\n",
            "\n",
            "You: Summarize the text: AskPlayStation CAN U HELP ME <BR> Very sorry for the delay! Is the issue still happening? Did you get any error? <BR> AskPlayStation it didnt go away so i just had to delete some games but i really didnt wat to do it but thanks anyway <BR> AskPlayStation even though i had 25gb free <BR> Do you see any kind of error message or error code? <BR> AskPlayStation No its fine now i managed to sort it out <BR> Feel free to tweet us any time. <BR> AskPlayStation thank you <BR>\n",
            "Chatbot: Advice on the issue is still happening.\n",
            "\n",
            "\n",
            "You: Summarize the text: Hi SafaricomCare. Kindly let me know how you can help me replace my line. Phone lost but still on. <BR> Hi, kindly get a new replacement card and contact us for further assistance. You will need to confirm your full name, ID .cont <BR> .number, last airtime top up and three frequently dialed numbers. RO <BR> SafaricomCare Am at a replacement center but lost the original card with pin n puk. Had changed pin too. <BR> SafaricomCare Please see DM. Thanks! <BR> Share a reachable contact number and the number to be replaced for assistance. JN <BR> We are dealing. AA <BR> SafaricomCare Waiting. Thanks. <BR>\n",
            "Chatbot: Customer is complaining about the phone lost but still on. Agent is asking for a new replacement card and contact them for further assistance.\n",
            "\n",
            "\n",
            "You: Summarize the text: AskPlayStation I have being trying to purchase Call Of Duty WW2 since yesterday, my credit card and PayPal keeps coming up invalid <BR> Thats odd. Try making the purchase via PC <LINK> Keep us posted! <BR> AskPlayStation Just tried on pc it didnt take it. Said check transaction history <BR> Have you tried deleting the card from the account just to make the purchase? <LINK> <BR> AskPlayStation Removed it and put in back in card and PayPal still got invalid message <BR> Thanks for trying that. Please make sure you are following us, so we can assist you better via a Direct Message. <BR> AskPlayStation Just followed you still trying its not working though <BR> Thanks for the follow. Please check your DMs for further instructions. <BR>\n",
            "Chatbot: Advice: I have been trying to purchase Call Of Duty WW2 since yesterday, my credit card and PayPal keeps coming up invalid.\n",
            "\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def chatbot():\n",
        "\n",
        "    print(\"Chatbot: All set!\")\n",
        "    print(\"I'm a text‑summarizing chatbot trained on the TweetSum dataset.\")\n",
        "    print(\"I'm ready to summarize any text you provide.\")\n",
        "    print(\"(Type 'quit' to stop)\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Check if the user wants to quit\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        response = generate_output(tokenizer, model, user_input)\n",
        "\n",
        "        # Print the generated text\n",
        "        print(f\"Chatbot: {response}\\n\\n\")\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "state": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}