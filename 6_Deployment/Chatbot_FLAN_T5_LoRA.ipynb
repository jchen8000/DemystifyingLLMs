{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/Chatbot_FLAN_T5_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc1wmPFS2tfq"
      },
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 6.11 Chatbot, Example of LLM-Powered Application\n",
        "\n",
        "### **Build a Chatbot with Local LLM plus a LoRA Adapter**\n",
        "\n",
        "This notebook demonstrates how to load and run a local LLM entirely on your machine (or a Google Colab GPU runtime), and enhance its capabilities using a LoRA fine‑tuned adapter. The LoRA adapter used here was created earlier in **Section 5.3**, where we fine‑tuned ```google/flan-t5-base``` on the TweetSum summarization dataset.\n",
        "\n",
        "To ensure reproducibility, the notebook downloads the fine‑tuned LoRA checkpoints directly from our GitHub repository.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Objectives**\n",
        "\n",
        "- **Local Model Execution**  \n",
        "  Load the ```google/flan-t5-base``` model directly from Hugging Face and run it locally on GPU using PyTorch.\n",
        "\n",
        "- **Apply a Fine‑Tuned LoRA Adapter**  \n",
        "  Attach the fine‑tuned LoRA weights (created in Section 5.3) to the base model using the ```PEFT``` library.\n",
        "\n",
        "- **Text Generation Pipeline**  \n",
        "  Generate responses—such as Tweet‑style summaries or general conversational output—fully offline without relying on external inference APIs.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ **Hardware & Compatibility Requirements**\n",
        "\n",
        "This notebook requires a GPU runtime (such as the Google Colab T4 GPU) because the model is executed locally using PyTorch with CUDA acceleration. Running on CPU is possible but not recommended due to performance limitations.\n",
        "\n",
        "This notebook is confirmed working as of **January 2026**.\n",
        "\n",
        "While the workflow reflects current best practices, deep‑learning libraries evolve rapidly, and future updates to Transformers, PEFT, or CUDA tooling may require adjustments to this notebook.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SDi0rf5XC2V-"
      },
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "    torch==2.9.0+cu126 \\\n",
        "    torchvision==0.24.0+cu126 \\\n",
        "    torchaudio==2.9.0+cu126 \\\n",
        "    transformers==4.57.6 \\\n",
        "    peft==0.18.1 \\\n",
        "    sentencepiece==0.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2QP9787zD3SX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "from transformers import T5Tokenizer\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import GenerationConfig\n",
        "from peft import PeftModel, PeftConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f69CP3hVP-d8"
      },
      "source": [
        "#### Download the LoRA model from github repo.  \n",
        "\n",
        "In Section 5.3, we fine‑tuned ```google/flan-t5-base``` model on the TweetSum summarization dataset, and we created the LoRA model called ```flan-t5-base_lora```. Now we download the LoRA model from github repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "woLmAhc8P-d9",
        "outputId": "ba9b8d22-9080-4b83-f342-67b7cd2f22a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'temp_repo'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 14 (delta 0), reused 10 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14/14), done.\n",
            "remote: Enumerating objects: 2, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 2 (delta 0), reused 1 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (2/2), 2.22 KiB | 2.22 MiB/s, done.\n",
            "/content/temp_repo\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (4/4), 2.66 MiB | 33.25 MiB/s, done.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!rm -rf lora_model\n",
        "\n",
        "!mkdir -p lora_model\n",
        "!git clone --depth 1 --filter=blob:none --sparse \\\n",
        "    https://github.com/jchen8000/DemystifyingLLMs.git temp_repo\n",
        "\n",
        "%cd temp_repo\n",
        "!git sparse-checkout set \"5_Fine-Tuning/outputs/flan-t5-base_lora\"\n",
        "%cd ..\n",
        "\n",
        "!mv temp_repo/5_Fine-Tuning/outputs/flan-t5-base_lora lora_model/\n",
        "!rm -rf temp_repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_9_AkdgP-d9"
      },
      "source": [
        "#### Load the Tokenizer, Base Model, and LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxrMp5UEDVBB",
        "outputId": "348f7c35-5565-459a-aec7-d95cada71201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters of the model: 249,347,328\n"
          ]
        }
      ],
      "source": [
        "base_model_name = 'google/flan-t5-base'\n",
        "lora_model_name = './lora_model/flan-t5-base_lora'\n",
        "\n",
        "def load_model(base_model_name, lora_model_name):\n",
        "\n",
        "    # Loading the base model and tokenizer into memory\n",
        "    tokenizer = T5Tokenizer.from_pretrained(base_model_name, legacy=True)\n",
        "    base_model = T5ForConditionalGeneration.from_pretrained(\n",
        "        base_model_name,\n",
        "        dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Applying LoRA adapter on top of the local base model\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        lora_model_name,\n",
        "        dtype=torch.bfloat16,\n",
        "        is_trainnable=False\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "tokenizer, model = load_model(base_model_name, lora_model_name)\n",
        "\n",
        "total_parameter = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Parameters of the model: {total_parameter:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mluYm8yP-d-"
      },
      "source": [
        "#### Define a Helper for Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IGz1lLhuP-d-"
      },
      "outputs": [],
      "source": [
        "def generate_output(tokenizer, model, input_text, max_length=200):\n",
        "    # Tokenize the input text and generate the model's output\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=GenerationConfig(max_new_tokens=200, num_beams=1) )\n",
        "\n",
        "    # Decode the generated tokens to a string\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdRkNXdJP-d_"
      },
      "source": [
        "#### Define and Run the Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oZgb7feCrRQ",
        "outputId": "da4005bd-3a27-4892-c4c1-cb743c1985c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot initialized.\n",
            "This is a text summarization chatbot, fine-tuned with TweetSum dataset.\n",
            "(type 'quit' to stop)\n",
            "\n",
            "You: Summarize the text: GooglePlayMusic I can no longer upload my music? Any suggestions on how to fix this? Thankyou! <BR> Hi Candace, wed like to help. Share more details and well see what we can do. Thanks! <BR> GooglePlayMusic Thank you so much for replying, Not sure why I cannot upload any more? Any thoughts on what I can do? Thanks again <BR> Mind sharing a screenshot of the issue? Well check it out. <BR> GooglePlayMusic I will do! Thank you Does it have to be in mp3 format or can music be mp4s as well? <BR> The music can be uploaded in MP3 and other formats. More info here <LINK> <BR> GooglePlayMusic Everything seems to be great now! Thanks again for your help, I love your service <BR> Youre very welcome! Were here if you have any other questions. <BR>\n",
            "Chatbot: Advice on how to fix this problem.\n",
            "\n",
            "You: Summarize the text: AmericanAir How are you still issuing paper vouchers redeemable by mail only? Please get your act together amp jump into the 21st century <BR> We do offer evouchers as well but in some instances we still need to issue paper vouchers. Thanks for understanding. <BR> AmericanAir surely you can have a better way to redeem them then to have to mail them in? <BR> They can be brought into an AA ticket counter for payment as well. <BR> AmericanAir Thats not exactly convenient. Go to an airport for future travel or pay for tracked shipping and spend a half hour to an hour on the phone <BR> Electronic vouchers are in the works. Our apologies for any inconvenience. <BR> AmericanAir Explain to me what to do now! In Manila, postal service wont ship the voucher because its considered money and no AA desk at NAIA. <BR> AmericanAir .and it needs to be sent by October 16th, Im beyond irritated now! <BR> You can ticket in person at Airesources, Inc. 4th floor, The Pilgrims Center Bldg, 758 P Ocampo ST. Malte, Manila 1004 MF 830a530p. <BR> AmericanAir .not to mention the voucher was for a flight you canceled for mechanical issues amp didnt compensate the full price as per regulations <BR> AmericanAir Will they accept the voucher there? I already have a record locator <BR> AmericanAir And those are my working hours. Im meant to take time off work for a voucher issued due to a flight cancellation caused by AA? <BR> AmericanAir Can I please get answers to my last questions? <BR> Youll be able to use your voucher at the address provided. Were so sorry for the inconvenience. <BR> AmericanAir So youre saying Im meant to take time off work? How about just scanning amp sending by email, ending this saga? How is AA still in oneworld? <BR> AmericanAir Is there really no way to just have this processed via email? <BR> Youll need to bring the voucher in person in order to use it. <BR> AmericanAir So helpful. <BR>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: Customer is asking about the vouchers and how are they still issuing paper vouchers redeemable by mail only?\n",
            "\n",
            "You: Summarize the text: AskPlayStation my ps 4 just stopped working and wont let me out of safe mode <BR> Do you have access to the option 5 Rebuild Database? <BR> AskPlayStation What is the option 5 rebuild database <BR> Scans the drive and creates a new database of all content. <BR> AskPlayStation I cant get to that stage because Im stuck here <LINK> <BR> Please turn off the console and disconnect all the cables from it for 3 minutes, then try again. <BR> AskPlayStation I tried that and it still didnt work <BR> Please follow us via Twitter and let us know so we can send you a DM with further instructions. Thanks! <BR> AskPlayStation Ive followed you <BR>\n",
            "Chatbot: Advice: The PS 4 just stopped working and wont let me out of safe mode.\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def chatbot():\n",
        "\n",
        "    print(\"Chatbot initialized.\\nThis is a text summarization chatbot, fine-tuned with TweetSum dataset.\\n(type 'quit' to stop)\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Check if the user wants to quit\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        response = generate_output(tokenizer, model, user_input)\n",
        "\n",
        "        # Print the generated text\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "state": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}