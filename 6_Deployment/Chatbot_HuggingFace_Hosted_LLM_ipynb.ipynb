{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/Chatbot_HuggingFace_Hosted_LLM_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK74JIJAraMz"
      },
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "# 6.11 Chatbot, Example of LLM-Powered Application\n",
        "\n",
        "### **Simple Chatbot Using Hugging Face Hosted LLM (Without LangChain)**\n",
        "\n",
        "This notebook demonstrates how to build a minimal chatbot using the **Hugging Face Inference Client** directly, without relying on LangChain or other orchestration frameworks.  \n",
        "It shows how to authenticate securely, send chat-formatted messages to a hosted large language model, maintain conversation history, and produce responses using a fully remote, cloud‚Äëhosted LLM.  \n",
        "The example uses the instruction‚Äëtuned **Qwen2.5‚Äë7B‚ÄëInstruct** model served through Hugging Face‚Äôs hosted inference API.\n",
        "\n",
        "A **Hugging Face access token** is required to run this notebook.  \n",
        "If you do not already have one, you can obtain it here:  \n",
        "üîó https://huggingface.co/docs/hub/security-tokens  \n",
        "The token should be stored securely as `HF_TOKEN` in **Colab Secrets** or another protected environment variable.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è **Compatibility Note**\n",
        "\n",
        "This notebook is confirmed working as of **January 2026**, including the currently available `Qwen/Qwen2.5-7B-Instruct` model. However, Hugging Face client libraries, model endpoints, and hosted model availability evolve rapidly, and future updates or model deprecations may require adjustments to this notebook. This example serves as a learning-oriented reference illustrating current best practices, not a permanently stable API contract.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "  huggingface_hub==0.36.0 \\\n",
        "  transformers==4.57.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRN563dwZz87"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "client = InferenceClient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE-IHnQym4Mu"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "def ask_model(message, history, temperature=0.7, max_tokens=512):\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}]\n",
        "    \n",
        "    for user_msg, bot_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "    \n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    try:\n",
        "        # We pass the model ID here instead of in the Client constructor\n",
        "        # This allows the 'auto' provider system to work best\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"System Busy: {str(e)}. Please try again in a moment.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMZQA2OqmRAI",
        "outputId": "530afac8-edd0-45a3-9d82-f10670e7a147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot initialized. You can start chatting now (type 'quit' to stop)!\n",
            "\n",
            "You: Hello\n",
            "Chatbot: Hello! How can I assist you today? If you have any questions or need help with something, feel free to ask. I'm here to help.\n",
            "\n",
            "You: How are you?\n",
            "Chatbot: I'm just a computer program, so I don't have feelings or emotions like humans do. I'm here to provide information and help you with your questions to the best of my ability. How can I assist you today?\n",
            "\n",
            "You: What are the top 5 largest cities in Canada?\n",
            "Chatbot: The top 5 largest cities in Canada by population (as of 2021) are:\n",
            "\n",
            "1. Toronto (Toronto-Durham Region CMA) - 6,417,516\n",
            "2. Montreal (CMA) - 4,340,395\n",
            "3. Vancouver (CMA) - 2,642,811\n",
            "4. Calgary (CMA) - 1,388,988\n",
            "5. Ottawa-Gatineau (CMA) - 1,429,629 (split between Ontario and Quebec)\n",
            "\n",
            "These population numbers are for the entire metropolitan areas, not just the city proper. The cities themselves have smaller populations.\n",
            "\n",
            "You: What is the next largest city?\n",
            "Chatbot: The next largest city in Canada, after the top 5, is Edmonton, with a population of 1,392,642 (as of 2021), according to Statistics Canada. This population number is for the entire Edmonton Metropolitan Region, not just the city proper.\n",
            "\n",
            "You: What is the population, again?\n",
            "Chatbot: The population of Edmonton, according to the latest data from Statistics Canada (as of 2021), is 1,392,642, for the entire Edmonton Metropolitan Region. This population number includes the city of Edmonton and its surrounding municipalities. The city of Edmonton itself has a population of 932,546.\n",
            "\n",
            "You: Where is it located?\n",
            "Chatbot: Edmonton is located in the province of Alberta, in western Canada. It is the capital city of Alberta and is situated on the North Saskatchewan River. It is known as the \"Festival City\" due to its many festivals and events held throughout the year.\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def chatbot():\n",
        "    print(\"Chatbot initialized. You can start chatting now (type 'quit' to stop)!\\n\")\n",
        "    history = []\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "            break\n",
        "\n",
        "        answer = ask_model(user_input, history)\n",
        "        print(f\"\\nAI: {answer}\\n\")\n",
        "        history.append([user_input, answer])\n",
        "\n",
        "chatbot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOiWKv5MTpnwBXioFdaEtO2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
