{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/Chatbot_HuggingFace_Hosted_LLM_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK74JIJAraMz"
      },
      "source": [
        "# 6.11 Chatbot, Example of LLM-Powered Application\n",
        "\n",
        "## Notebook Description & Compatibility Notice\n",
        "\n",
        "This notebook demonstrates how to build a lightweight conversational chatbot using **Hugging Face Inference APIs** via the `huggingface_hub` Python SDK.\n",
        "\n",
        "### What this notebook does\n",
        "- Uses **Hugging Face hosted large language models** for text generation\n",
        "- Authenticates requests using a **Hugging Face Access Token**\n",
        "- Leverages the `InferenceClient` with **automatic provider selection**, which dynamically routes requests to the best available inference backend to reduce 404/503 errors\n",
        "- Implements a simple interactive chat loop with conversation history and configurable generation parameters (temperature and max tokens)\n",
        "\n",
        "### Authentication Requirement\n",
        "This script **requires a Hugging Face Access Token** to run.\n",
        "\n",
        "- Create a token at: https://huggingface.co/settings/tokens  \n",
        "- Store the token securely (e.g., as a Colab secret named `HF_TOKEN`)\n",
        "- The token is retrieved at runtime and is **not hard-coded** in the notebook\n",
        "\n",
        "Without a valid token, inference requests will fail.\n",
        "\n",
        "### Model Notes\n",
        "- The notebook uses the model: `Qwen/Qwen2.5-7B-Instruct`\n",
        "- This model is stable and high-performance **as of 2026**\n",
        "- The model ID can be changed to any compatible Hugging Face chat model\n",
        "What this script does\n",
        "\n",
        "### ⚠️ Compatibility & Future-Proofing Notice\n",
        "This notebook and its code are **tested and functional in 2026**. However, Hugging Face continuously updates its:\n",
        "- Models and model availability\n",
        "- API endpoints and request formats\n",
        "- SDK behavior (`huggingface_hub`, `transformers`)\n",
        "- Hosting providers and inference backends\n",
        "- Authentication and usage policies\n",
        "\n",
        "As a result, this notebook **may stop working in the future without modification**.\n",
        "\n",
        "Potential breaking changes include:\n",
        "- Model deprecation or renaming\n",
        "- Changes to the inference API interface\n",
        "- Authentication or quota policy updates\n",
        "- SDK version incompatibilities\n",
        "\n",
        "This notebook should be treated as a **reference implementation**, not a permanently guaranteed interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install \\\n",
        "  huggingface_hub==0.36.0 \\\n",
        "  transformers==4.57.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRN563dwZz87"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import userdata\n",
        "\n",
        "HuggingFaceToken = userdata.get('HF_TOKEN')\n",
        "\n",
        "client = InferenceClient(\n",
        "    token=HuggingFaceToken\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oE-IHnQym4Mu"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "def ask_model(message, history, temperature=0.7, max_tokens=512):\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}]\n",
        "    \n",
        "    for user_msg, bot_msg in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "    \n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    try:\n",
        "        # We pass the model ID here instead of in the Client constructor\n",
        "        # This allows the 'auto' provider system to work best\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL_ID,\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"System Busy: {str(e)}. Please try again in a moment.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMZQA2OqmRAI",
        "outputId": "530afac8-edd0-45a3-9d82-f10670e7a147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot initialized. You can start chatting now (type 'quit' to stop)!\n",
            "\n",
            "You: Hello\n",
            "Chatbot: Hello! How can I assist you today? If you have any questions or need help with something, feel free to ask. I'm here to help.\n",
            "\n",
            "You: How are you?\n",
            "Chatbot: I'm just a computer program, so I don't have feelings or emotions like humans do. I'm here to provide information and help you with your questions to the best of my ability. How can I assist you today?\n",
            "\n",
            "You: What are the top 5 largest cities in Canada?\n",
            "Chatbot: The top 5 largest cities in Canada by population (as of 2021) are:\n",
            "\n",
            "1. Toronto (Toronto-Durham Region CMA) - 6,417,516\n",
            "2. Montreal (CMA) - 4,340,395\n",
            "3. Vancouver (CMA) - 2,642,811\n",
            "4. Calgary (CMA) - 1,388,988\n",
            "5. Ottawa-Gatineau (CMA) - 1,429,629 (split between Ontario and Quebec)\n",
            "\n",
            "These population numbers are for the entire metropolitan areas, not just the city proper. The cities themselves have smaller populations.\n",
            "\n",
            "You: What is the next largest city?\n",
            "Chatbot: The next largest city in Canada, after the top 5, is Edmonton, with a population of 1,392,642 (as of 2021), according to Statistics Canada. This population number is for the entire Edmonton Metropolitan Region, not just the city proper.\n",
            "\n",
            "You: What is the population, again?\n",
            "Chatbot: The population of Edmonton, according to the latest data from Statistics Canada (as of 2021), is 1,392,642, for the entire Edmonton Metropolitan Region. This population number includes the city of Edmonton and its surrounding municipalities. The city of Edmonton itself has a population of 932,546.\n",
            "\n",
            "You: Where is it located?\n",
            "Chatbot: Edmonton is located in the province of Alberta, in western Canada. It is the capital city of Alberta and is situated on the North Saskatchewan River. It is known as the \"Festival City\" due to its many festivals and events held throughout the year.\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def chatbot():\n",
        "    print(f\"--- Chatbot Live ({MODEL_ID}) ---\")\n",
        "    print(\"Type 'quit' to exit.\\n\")\n",
        "    history = []\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "            break\n",
        "\n",
        "        answer = ask_model(user_input, history)\n",
        "        print(f\"\\nAI: {answer}\\n\")\n",
        "        history.append([user_input, answer])\n",
        "\n",
        "chatbot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOiWKv5MTpnwBXioFdaEtO2",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
