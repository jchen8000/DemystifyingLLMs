{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ao_qlDHlDEv"
      },
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 6.10 LangChain\n",
        "\n",
        "### **Building a Simple Pipeline with LangChain and HuggingFace Endpoints**\n",
        "\n",
        "This notebook demonstrates how to build a text classification pipeline using **LangChain** and the **HuggingFace Inference API**. Specifically, we use the a model in HuggingFace to categorize customer reviews into **Positive**, **Negative**, or **Neutral** sentiments.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Objectives**\n",
        "* **Prompt Templating**: Using LangChain's `PromptTemplate` to standardize instructions for the LLM.\n",
        "* **LLM Integration**: Connecting to Hugging Face's hosted inference endpoints without needing local GPU resources.\n",
        "* **Batch Processing**: Efficiently running multiple inputs through a chain simultaneously using the `.batch()` method.\n",
        "\n",
        "---\n",
        "\n",
        "###⚠️ **Compatibility Note**\n",
        "\n",
        "This notebook is confirmed working as of **January 2026**, including the currently available ```google/gemma-2-9b-it``` model. However, LangChain, Hugging Face APIs, and hosted models evolve rapidly, and future updates or model deprecations may require adjustments to this notebook.\n",
        "\n",
        "**This example is for learning and reference purposes and reflects the current best practices, but the APIs used here may change over time.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro859Kni5Npm"
      },
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "  langchain==1.2.3 \\\n",
        "  langchain-core==1.2.6 \\\n",
        "  langchain-huggingface==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HHrvHxH5LV8v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Udh67shTNMK"
      },
      "source": [
        "### Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOBc8iP544iD",
        "outputId": "274b0770-0af6-49e2-9669-3b294ea0cc52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            "\n",
            "The customer review is: The shipping was quick and the item was perfect. Totally satisfied!\n",
            "Please classify it as Positive, Negative, or Neutral. Reply with the classification only.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define a simple prompt template\n",
        "template = \"\"\"\n",
        "The customer review is: {review}\n",
        "Please classify it as Positive, Negative, or Neutral. Reply with the classification only.\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "review_text = \"The shipping was quick and the item was perfect. Totally satisfied!\"\n",
        "\n",
        "prompt_text = prompt.format(review=review_text)\n",
        "print(f\"Prompt:\\n{prompt_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rboOXgNjZ-gj"
      },
      "source": [
        "### Access a LLM model on HuggingFace hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUrQrwcigrs1"
      },
      "source": [
        "### Inference with a single prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmYFS-dXZ5RR",
        "outputId": "d49963b7-459c-4e7b-d326-685b6403c834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review text: The shipping was quick and the item was perfect. Totally satisfied!\n",
            "Positive \n",
            "\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-9b-it\",\n",
        "    max_new_tokens=20,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "chain = prompt | chat_model | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\"review\": review_text})\n",
        "\n",
        "print(f\"Review: {review_text}\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzaTbYPLgyI4"
      },
      "source": [
        "### Inference with multiple prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3buIQpfDd33R",
        "outputId": "a9c8053f-8c22-409a-df41-a119d8637433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Sentiment Analysis Results ---\n",
            "Review: The shipping was quick and the item was perfect. Totally satisfied!\n",
            "Sentiment: Positive\n",
            "\n",
            "Review: The restaurant was terrible, and the service was even worse. Not going back there again.\n",
            "Sentiment: Negative\n",
            "\n",
            "Review: The new update is almost useless, the new features seem not relevant to our business flow.\n",
            "Sentiment: Negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "reviews = [\n",
        "    {'review': \"The shipping was quick and the item was perfect. Totally satisfied!\"},\n",
        "    {'review': \"The restaurant was terrible, and the service was even worse. Not going back there again.\"},\n",
        "    {'review': \"The new update is almost useless, the new features seem not relevant to our business flow.\"},\n",
        "]\n",
        "results = chain.batch(reviews)\n",
        "\n",
        "print(\"--- Sentiment Analysis Results ---\")\n",
        "for original, classification in zip(reviews, results):\n",
        "    print(f\"Review: {original['review']}\")\n",
        "    print(f\"Sentiment: {classification.strip()}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMDv5Q0IrjBaWjoejZga/Up",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
