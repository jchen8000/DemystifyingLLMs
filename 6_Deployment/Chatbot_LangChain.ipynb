{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/Chatbot_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upspEbuBxYTd"
      },
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 6.11 Chatbot, Example of LLM-Powered Application\n",
        "\n",
        "### **Building a Simple Chatbot with LangChain and HuggingFace Endpoints**\n",
        "\n",
        "This notebook demonstrates how to build a lightweight chatbot using **LangChain‚Äôs LCEL pipeline** and **HuggingFace‚Äôs Inference Endpoint API**.  \n",
        "It shows how to authenticate securely, construct a prompt-driven pipeline, interface with a remotely hosted model, and build a simple interactive text-based chatbot.  \n",
        "\n",
        "A **Hugging Face access token** is required to run this notebook. \n",
        "If you do not already have one, you can obtain it at:  \n",
        "üîó https://huggingface.co/docs/hub/security-tokens  \n",
        "The token should be saved as `HF_TOKEN` in Colab Secrets or another secured environment variable before running the example.\n",
        "\n",
        "---\n",
        "\n",
        "###‚ö†Ô∏è **Compatibility Note**\n",
        "\n",
        "This notebook is confirmed working as of **January 2026**, including the currently available `google/gemma-2-9b-it` model. However, LangChain, Hugging Face APIs, and hosted models evolve rapidly, and future updates or model deprecations may require adjustments to this notebook.\n",
        "\n",
        "**This example is for learning and reference purposes and reflects the current best practices, but the APIs used here may change over time.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ3585RWsgnk",
        "outputId": "d80e9722-9177-4d72-8d76-4ac053cc586c"
      },
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "  langchain==1.2.3 \\\n",
        "  langchain-core==1.2.6 \\\n",
        "  langchain-huggingface==1.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxLDptd4tNeW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Load HF token from Colab\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5Q3NSnHtQQ1"
      },
      "outputs": [],
      "source": [
        "# Build prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Please answer the question: {question}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NEW correct HF endpoint wrapper\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-9b-it\"\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)\n",
        "chain = prompt | chat_model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbJRxaZcujF3",
        "outputId": "3ddb5566-3da5-4e70-97df-34a0318ea5a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot initialized. You can start chatting now (type 'quit' to stop)!\n",
            "\n",
            "You: How are you\n",
            "Chatbot: good\n",
            "\n",
            "You: Which is the capital city of Japan?\n",
            "Chatbot: tokyo\n",
            "\n",
            "You: What do turtles eat?\n",
            "Chatbot: mollusks\n",
            "\n",
            "You: How much do elephants weight at birth?\n",
            "Chatbot: 240‚Äì400 lbs\n",
            "\n",
            "You: How much time to penguins spend on land?\n",
            "Chatbot: a few hours\n",
            "\n",
            "You: Thanks\n",
            "Chatbot: thank you\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def chatbot():\n",
        "\n",
        "    print(\"Chatbot initialized. You can start chatting now (type 'quit' to stop)!\\n\")\n",
        "\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Check if the user wants to quit\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        answer = chain.invoke({\"question\": user_input})\n",
        "        print(f\"Chatbot: {answer}\\n\")\n",
        "\n",
        "# Run the chatbot\n",
        "chatbot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNd3ugIjc8s4COl/Hpd45VN",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
