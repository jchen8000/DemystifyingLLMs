{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAhmO98EQKByIZri+iwTFJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/RAG_LangChain_Groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 6.11 Retrieval Augmented Generation (RAG) Application\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is one of the most powerful applications enabled by LLMs, it's a technique for augmenting LLM knowledge with additional data.\n",
        "\n",
        "Large Language Models (LLMs) are indeed powerful tools for natural language processing (NLP). However, their knowledge is confined to the public data they were trained on. This means they might not be aware of private data, domain specific data or any new information introduced after their training cutoff date.\n",
        "\n",
        "To build AI applications that can reason about such private, domain-specific or up-to-date data, it’s important to augment the model’s knowledge with the specific information it needs. This is what RAG is trying to achieve. RAG involves retrieving the relevant information and incorporating it into the model’s prompt, enabling the LLMs to generate responses based on the most current and specific data available.\n",
        "\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. **Retrieval**:\n",
        "The model analyzes the user's input and retrieves relevant information from a vast knowledge base.\n",
        "This includes documents, conversations, or other textual sources.\n",
        "\n",
        "2. **Augmentation**:\n",
        "The retrieved information is augmented with additional context and knowledge.\n",
        "This includes the user's intent and domain-specific knowledge.\n",
        "\n",
        "3. **Generation**:\n",
        "The augmented information is used to generate a comprehensive and informative response. The model combines the retrieved content with the new context to create a coherent and relevant output."
      ],
      "metadata": {
        "id": "sNmZBiDG9b_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Install the packages and import them"
      ],
      "metadata": {
        "id": "CEW82hQCEpes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_chroma\n",
        "!pip install pypdf\n",
        "!pip install -U langchain-huggingface\n",
        "!pip install FAISS-gpu\n",
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "id": "OAkhHfkYVZ9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZyClqdToU3iH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Indexing\n",
        "\n",
        "Indexing is an important process for a RAG application, ensuring the efficient retrieval of the most relevant information.\n",
        "It includes the following steps:\n",
        "\n",
        "* **Load Documents**: The first step is to load the documents or\n",
        "data you want to use. This can be private, domain-specific or up-to-data data including text files, PDFs, web pages, or any other relevant sources.\n",
        "* **Split into Small Chunks**: Once the documents are loaded, they are split into smaller, manageable chunks. This is important because smaller chunks are easier to process and retrieve. The splitting can be based on sentences, paragraphs, or fixed token lengths.\n",
        "* **Embedding**: Each chunk of text is then converted into a vector representation using an embedding model. These embeddings capture the semantic meaning of the text in a numerical format, making it easier for the system to understand and compare different pieces of information.\n",
        "* **Store in a Vector Database**: The final step is to store these vector embeddings in a vector database. This specialized database is optimized for storing and retrieving high-dimensional vectors, allowing for efficient similarity searches.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KMP5unl_19FF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Document\n",
        "\n",
        "This example is to load a PDF file from a URL. First, we download the PDF to a local file.\n",
        "\n",
        "```PyPDFLoader()``` function provides a way to load and extract text from PDF documents. It comes with ```langchain_community.document_loaders``` libary.\n",
        "\n",
        "```loader.load()``` is to load the PDF document and extract its text content. The result ```document``` is a string or a list of strings representing the text content of the PDF document.\n"
      ],
      "metadata": {
        "id": "aufpFjCHICYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a PDF file\n",
        "url = \"https://arxiv.org/pdf/1706.03762\"\n",
        "doc_name = \"Attention_Is_All_You_Need.pdf\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(doc_name, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Load the PDF file\n",
        "loader = PyPDFLoader(doc_name)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "mCHUwgXvVMdZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split into small trunks\n",
        "\n",
        "```RecursiveCharacterTextSplitter``` is a class that implements a text splitting algorithm, specifically designed for RAG applications. Where ```chunk_size``` specifies the maximum size of each chunk (or split) in characters. ```chunk_overlap``` specifies the amount of overlap between consecutive chunks. In this case, each chunk will overlap with the previous chunk by 200 characters.\n",
        "\n",
        "The purpose of splitting the documents is to create a set of smaller, more manageable pieces of text that can be embedded and indexed in the vector database."
      ],
      "metadata": {
        "id": "KzOryalPIN_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "9ec1RhR_Zfuc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, check how many splits in total, and the content of any given split."
      ],
      "metadata": {
        "id": "yEBhUJReK81v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print('splits:', len(splits))\n",
        "# print('splits[20] metadata:', splits[20].metadata)\n",
        "# print('splits[20] content:', splits[20].page_content)"
      ],
      "metadata": {
        "id": "Ya-EWVYkyU7s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding\n",
        "\n",
        "Load an embedding model from Hugging Face Transformers library.\n",
        "\n",
        "```HuggingFaceEmbeddings``` is a class that provides an interface for generating embeddings from a pre-trained language model, which in this example is *distilbert-base-uncased* model.\n",
        "\n",
        "Reference Section 2.8 and Section 3.2 of the book ***Demystifying Large Language Models*** for more details about embedding.\n"
      ],
      "metadata": {
        "id": "XsAts0RWIrNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"distilbert-base-uncased\", encode_kwargs={\"normalize_embeddings\": True})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azWLgJaXafYm",
        "outputId": "1fe3dd42-b3df-4595-b06e-e01231d64234"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name distilbert-base-uncased. Creating a new one with mean pooling.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Store in a Vector Database\n",
        "\n",
        "This example uses FAISS (Facebook AI Similarity Search) as the vector database, the function ```FAISS.from_documents(splits, embeddings)``` is to create a FAISS index from the splitted texts and their corresponding embeddings. When the below code is executed, FAISS creates an index that maps each splitted text to its corresponding embedding. This index allows for efficient similarity search, clustering, and other operations on the embeddings.\n",
        "\n",
        "Optionally, you can save the vector database into a local folder.\n",
        "\n",
        "Reference Section 6.9 of the book ***Demystifying Large Language Models***"
      ],
      "metadata": {
        "id": "jW_iHI6HI6m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector store\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "# Save the documents and embeddings\n",
        "vectorstore.save_local(\"vectorstore.db\")"
      ],
      "metadata": {
        "id": "P9z-6iovnJs0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the size of the vectorstore.db\n",
        "!du -sh vectorstore.db\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GptIwjU1COn",
        "outputId": "d3612adc-f839-4520-ce74-91dd1683d313"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "216K\tvectorstore.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Retrieval and Generation\n",
        "\n",
        "Retrieval and Generation are essential processes in a RAG application, enabling it to deliver more precise and informed responses by leveraging the power of LLMs along with the private, domain-specific, and up-to-date data stored in the vector database.\n",
        "\n",
        "\n",
        "* **Retrieve Related Information**: When a user submits a query, the system first searches the vector database to find the most relevant chunks of information. These chunks are retrieved based on their semantic similarity to the query.\n",
        "* **Augment the Prompt**: The retrieved information is then used to augment the original query. This is often done using a framework like LangChain, which helps in seamlessly integrating the additional context into the prompt. This step ensures that the language model has all the necessary information to generate a more accurate and contextually relevant response.\n",
        "* **Invoke the LLM**: Finally, the augmented prompt is passed to the Large Language Model (LLM). The LLM processes the combined input and generates a response that leverages both the original query and the retrieved information."
      ],
      "metadata": {
        "id": "Tw9FK1PrDc77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieve Related Information\n",
        "\n",
        "```vectorstore.as_retriever()``` function is used for retrieving relevant information from a vector database, based on their semantic similarity.\n",
        "\n",
        "```format_docs(docs)``` function is defined for formatting the retrieved information.\n",
        "\n"
      ],
      "metadata": {
        "id": "VcVopGD4LDyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 8})\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "fZU4SutVj1fA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can issue a user query and call ```retriever.invoke(user_query)``` function to check what information is retrived from the ```retriever```\n"
      ],
      "metadata": {
        "id": "PrIqkp0ilHy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# user_query = \"What is Attention\"\n",
        "# retrieved_docs = retriever.invoke(user_query)\n",
        "# print(len(retrieved_docs))\n",
        "# print(format_docs(retrieved_docs))"
      ],
      "metadata": {
        "id": "B8UqloIdDXHD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Augment the Prompt\n",
        "\n",
        "Create a custom prompt template using ```PromptTemplate.from_template(template)``` function to format the context and user query. The prompt explicitly tells the LLM to answer the question based on the provided context.\n",
        "\n",
        "Then build a model with LangChain that includes:\n",
        "* LLM (Groq with model=\"llama3-70b-8192\" in this example)\n",
        "* Prompt template\n",
        "* Retrived context from the vector database\n",
        "* User's query\n",
        "\n",
        "The code will output a concise and informative answer to the user's query, based on the provided context and the language model's understanding of the conversation.\n",
        "\n",
        "Please note, an API key from Groq Cloud is required for this example. The API key ***GROQ_API_KEY*** is stored in the Secret of Google Colab. Please see other code examples in this chapter for how to do it.\n"
      ],
      "metadata": {
        "id": "Kwnq_S4BL6f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "llm = ChatGroq(groq_api_key=userdata.get('GROQ_API_KEY'), model=\"llama3-70b-8192\")\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ],
      "metadata": {
        "id": "Pr9OKqjiIGd4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Invoke the LLM\n",
        "\n",
        "Build a simple conversational chatbot."
      ],
      "metadata": {
        "id": "wlrhu7IexqN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_question = input(\"You: \")\n",
        "\n",
        "    if user_question.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    response = rag_chain.invoke(user_question)\n",
        "    print(\"Chatbot:\", response)\n",
        "    print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zgjU08G11tC",
        "outputId": "720a7f31-f235-4ad9-931b-3a839adfdb34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: What is Attention?\n",
            "Chatbot: According to the provided context, attention is an attention mechanism that relates different positions of a single sequence in order to compute a representation of the sequence. It's also referred to as intra-attention. Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: What is Scaled Dot-Product Attention?\n",
            "Chatbot: According to the provided context, Scaled Dot-Product Attention is an attention mechanism that computes the compatibility function using dot products, scaled by a factor of `1/√dk`, where `d` is the dimensionality of the keys, to counteract the effect of large dot products pushing the softmax function into regions with extremely small gradients.\n",
            "\n",
            "Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: What are Encoder and Decoder?\n",
            "Chatbot: According to the text, the Encoder and Decoder are components of the Transformer model architecture.\n",
            "\n",
            "The Encoder is composed of a stack of N=6 identical layers, each layer having two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. The Encoder takes in input tokens and outputs a sequence of equal length.\n",
            "\n",
            "The Decoder is also composed of a stack of N=6 identical layers, each layer having three sub-layers: a multi-head self-attention mechanism, a multi-head attention over the output of the Encoder stack, and a simple, position-wise fully connected feed-forward network. The Decoder outputs predicted next-token probabilities.\n",
            "\n",
            "Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: Describe more about the Transformer.\n",
            "Chatbot: The Transformer is a neural sequence transduction model that relies entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. It follows an encoder-decoder architecture, where the encoder maps an input sequence of symbol representations to a sequence of continuous representations, and the decoder generates an output sequence based on those representations. The model uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. It employs a residual connection and layer normalization around each sub-layer.\n",
            "\n",
            "The Transformer uses multi-head self-attention mechanisms, which allow it to attend to different aspects of the input sequence simultaneously. The attention function is computed as a weighted sum of the values, where the weights are computed based on the query and keys. The model also uses a sinusoidal positional embedding to preserve the order of the sequence.\n",
            "\n",
            "The authors of the paper experimented with different variations of the model, including using learned positional embeddings instead of sinusoidal embeddings, and found that the two versions produced nearly identical results. They also observed that the model's performance improved with bigger models and dropout, and that the attention distributions from the model were interpretable and exhibited behavior related to the syntactic and semantic structure of the sentences.\n",
            "\n",
            "Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: Why use self-attention?\n",
            "Chatbot: The authors of the paper suggest that self-attention is used because it allows for parallelization of computation, reducing the total computational complexity per layer, and it also allows for modeling dependencies without regard to their distance in the input or output sequences. Additionally, self-attention can yield more interpretable models, as individual attention heads can be seen to learn different tasks related to the syntactic and semantic structure of sentences. Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: quit\n"
          ]
        }
      ]
    }
  ]
}