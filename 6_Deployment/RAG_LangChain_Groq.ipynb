{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/RAG_LangChain_Groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNmZBiDG9b_s"
      },
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 6.11 Retrieval Augmented Generation (RAG) Application\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is one of the most powerful applications enabled by LLMs, it's a technique for augmenting LLM knowledge with additional data.\n",
        "\n",
        "Large Language Models (LLMs) are indeed powerful tools for natural language processing (NLP). However, their knowledge is confined to the public data they were trained on. This means they might not be aware of private data, domain specific data or any new information introduced after their training cutoff date.\n",
        "\n",
        "To build AI applications that can reason about such private, domain-specific or up-to-date data, it’s important to augment the model’s knowledge with the specific information it needs. This is what RAG is trying to achieve. RAG involves retrieving the relevant information and incorporating it into the model’s prompt, enabling the LLMs to generate responses based on the most current and specific data available.\n",
        "\n",
        "\n",
        "### **How it works:**\n",
        "\n",
        "1. **Retrieval**:\n",
        "The model analyzes the user's input and retrieves relevant information from a vast knowledge base.\n",
        "This includes documents, conversations, or other textual sources.\n",
        "\n",
        "2. **Augmentation**:\n",
        "The retrieved information is augmented with additional context and knowledge.\n",
        "This includes the user's intent and domain-specific knowledge.\n",
        "\n",
        "3. **Generation**:\n",
        "The augmented information is used to generate a comprehensive and informative response. The model combines the retrieved content with the new context to create a coherent and relevant output.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ **Compatibility Note**\n",
        "\n",
        "This notebook is confirmed working as of **January 2026** in Google Colab. However, LangChain, HuggingFace, as well as other related APIs evolve rapidly, and future updates or model deprecations may require adjustments to this notebook.\n",
        "\n",
        "**This example is for learning and reference purposes and reflects the current best practices, but the APIs used here may change over time.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEW82hQCEpes"
      },
      "source": [
        "### 0. Install the packages and import them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OAkhHfkYVZ9L"
      },
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "  langchain==1.2.3 \\\n",
        "  langchain-core==1.2.6 \\\n",
        "  langchain-community==0.4.1 \\\n",
        "  langchain-huggingface==1.2.0 \\\n",
        "  langchain-chroma==1.1.0 \\\n",
        "  langchain_groq==1.1.1 \\\n",
        "  faiss-cpu==1.13.2 \\\n",
        "  pypdf===6.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZyClqdToU3iH",
        "outputId": "ef00ade0-8e6d-4aee-e5a4-1f7541e03df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMP5unl_19FF"
      },
      "source": [
        "### 1. Indexing\n",
        "\n",
        "Indexing is an important process for a RAG application, ensuring the efficient retrieval of the most relevant information.\n",
        "It includes the following steps:\n",
        "\n",
        "* **Load Documents**: The first step is to load the documents or\n",
        "data you want to use. This can be private, domain-specific or up-to-data data including text files, PDFs, web pages, or any other relevant sources.\n",
        "* **Split into Small Chunks**: Once the documents are loaded, they are split into smaller, manageable chunks. This is important because smaller chunks are easier to process and retrieve. The splitting can be based on sentences, paragraphs, or fixed token lengths.\n",
        "* **Embedding**: Each chunk of text is then converted into a vector representation using an embedding model. These embeddings capture the semantic meaning of the text in a numerical format, making it easier for the system to understand and compare different pieces of information.\n",
        "* **Store in a Vector Database**: The final step is to store these vector embeddings in a vector database. This specialized database is optimized for storing and retrieving high-dimensional vectors, allowing for efficient similarity searches.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aufpFjCHICYw"
      },
      "source": [
        "#### Load Document\n",
        "\n",
        "This example is to load a PDF file from a URL. First, we download the PDF to a local file.\n",
        "\n",
        "```PyPDFLoader()``` function provides a way to load and extract text from PDF documents. It comes with ```langchain_community.document_loaders``` libary.\n",
        "\n",
        "```loader.load()``` is to load the PDF document and extract its text content. The result ```document``` is a string or a list of strings representing the text content of the PDF document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mCHUwgXvVMdZ"
      },
      "outputs": [],
      "source": [
        "# Download a PDF file\n",
        "url = \"https://arxiv.org/pdf/1706.03762\"\n",
        "doc_name = \"Attention_Is_All_You_Need.pdf\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(doc_name, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Load the PDF file\n",
        "loader = PyPDFLoader(doc_name)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzOryalPIN_t"
      },
      "source": [
        "#### Split into small trunks\n",
        "\n",
        "```RecursiveCharacterTextSplitter``` is a class that implements a text splitting algorithm, specifically designed for RAG applications. Where ```chunk_size``` specifies the maximum size of each chunk (or split) in characters. ```chunk_overlap``` specifies the amount of overlap between consecutive chunks. In this case, each chunk will overlap with the previous chunk by 200 characters.\n",
        "\n",
        "The purpose of splitting the documents is to create a set of smaller, more manageable pieces of text that can be embedded and indexed in the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9ec1RhR_Zfuc"
      },
      "outputs": [],
      "source": [
        "# Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEBhUJReK81v"
      },
      "source": [
        "Optionally, check how many splits in total, and the content of any given split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ya-EWVYkyU7s"
      },
      "outputs": [],
      "source": [
        "# print('splits:', len(splits))\n",
        "# print('splits[20] metadata:', splits[20].metadata)\n",
        "# print('splits[20] content:', splits[20].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsAts0RWIrNg"
      },
      "source": [
        "#### Embedding\n",
        "\n",
        "Load an embedding model from Hugging Face Transformers library.\n",
        "\n",
        "```HuggingFaceEmbeddings``` is a class that provides an interface for generating embeddings from a pre-trained language model, which in this example is *bert-base-uncased* model.\n",
        "\n",
        "Reference Section 2.8 and Section 3.2 of the book [***Demystifying Large Language Models***](https://github.com/jchen8000/DemystifyingLLMs/) for more details about embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "azWLgJaXafYm"
      },
      "outputs": [],
      "source": [
        "# Load the embedding model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW_iHI6HI6m3"
      },
      "source": [
        "#### Store in a Vector Database\n",
        "\n",
        "This example uses FAISS (Facebook AI Similarity Search) as the vector database, the function ```FAISS.from_documents(splits, embeddings)``` is to create a FAISS index from the splitted texts and their corresponding embeddings. When the below code is executed, FAISS creates an index that maps each splitted text to its corresponding embedding. This index allows for efficient similarity search, clustering, and other operations on the embeddings.\n",
        "\n",
        "Optionally, you can save the vector database into a local folder.\n",
        "\n",
        "Reference Section 6.9 of the book [***Demystifying Large Language Models***](https://github.com/jchen8000/DemystifyingLLMs/) for more details on vector database. And reference https://github.com/facebookresearch/faiss for FAISS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P9z-6iovnJs0"
      },
      "outputs": [],
      "source": [
        "# Create a vector store\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "# Save the documents and embeddings\n",
        "vectorstore.save_local(\"vectorstore.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GptIwjU1COn",
        "outputId": "0ee27c9a-34cc-4b86-f43d-705c7af0939c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140K\tvectorstore.db\n"
          ]
        }
      ],
      "source": [
        "# Check the size of the vectorstore.db\n",
        "!du -sh vectorstore.db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw9FK1PrDc77"
      },
      "source": [
        "### 2. Retrieval and Generation\n",
        "\n",
        "Retrieval and Generation are essential processes in a RAG application, enabling it to deliver more precise and informed responses by leveraging the power of LLMs along with the private, domain-specific, and up-to-date data stored in the vector database.\n",
        "\n",
        "\n",
        "* **Retrieve Related Information**: When a user submits a query, the system first searches the vector database to find the most relevant chunks of information. These chunks are retrieved based on their semantic similarity to the query.\n",
        "* **Augment the Prompt**: The retrieved information is then used to augment the original query. This is often done using a framework like LangChain, which helps in seamlessly integrating the additional context into the prompt. This step ensures that the language model has all the necessary information to generate a more accurate and contextually relevant response.\n",
        "* **Invoke the LLM**: Finally, the augmented prompt is passed to the Large Language Model (LLM). The LLM processes the combined input and generates a response that leverages both the original query and the retrieved information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcVopGD4LDyE"
      },
      "source": [
        "#### Retrieve Related Information\n",
        "\n",
        "```vectorstore.as_retriever()``` function is used for retrieving relevant information from a vector database, based on their semantic similarity.\n",
        "\n",
        "```format_docs(docs)``` function is defined for formatting the retrieved information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fZU4SutVj1fA"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 8})\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrIqkp0ilHy9"
      },
      "source": [
        "Optionally, you can issue a user query and call ```retriever.invoke(user_query)``` function to check what information is retrived from the ```retriever```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B8UqloIdDXHD"
      },
      "outputs": [],
      "source": [
        "# user_query = \"What is Attention\"\n",
        "# retrieved_docs = retriever.invoke(user_query)\n",
        "# print(len(retrieved_docs))\n",
        "# print(format_docs(retrieved_docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwnq_S4BL6f6"
      },
      "source": [
        "#### Augment the Prompt\n",
        "\n",
        "Create a custom prompt template using ```PromptTemplate.from_template(template)``` function to format the context and user query. The prompt explicitly tells the LLM to answer the question based on the provided context.\n",
        "\n",
        "Then build a model with LangChain that includes:\n",
        "* LLM (Groq with model=\"llama3-70b-8192\" in this example)\n",
        "* Prompt template\n",
        "* Retrived context from the vector database\n",
        "* User's query\n",
        "\n",
        "The code will output a concise and informative answer to the user's query, based on the provided context and the language model's understanding of the conversation.\n",
        "\n",
        "Please note, an API key from Groq Cloud is required for this example. The API key ***GROQ_API_KEY*** is stored in the Secret of Google Colab. Please see other code examples in this chapter for how to do it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Pr9OKqjiIGd4"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "GROP_MODEL = \"llama-3.3-70b-versatile\"\n",
        "llm = ChatGroq(groq_api_key=userdata.get('GROQ_API_KEY'), model=GROP_MODEL)\n",
        "\n",
        "template = \\\n",
        "\"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlrhu7IexqN3"
      },
      "source": [
        "#### Invoke the LLM\n",
        "\n",
        "Build a simple conversational chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zgjU08G11tC",
        "outputId": "09d547a3-2b50-48f8-fc4d-37f8b046309e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot initialized.\n",
            "The chatbot is to help answer anything related to Attention_Is_All_You_Need.pdf.\n",
            "(type 'quit' to stop)\n",
            "\n",
            "You: What is Transformer?\n",
            "Chatbot: The Transformer is a model architecture that relies entirely on an attention mechanism to draw global dependencies between input and output, eschewing recurrence. It allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for a relatively short period of time.\n",
            "\n",
            "thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: What is Attention?\n",
            "Chatbot: Attention is a mechanism that allows a model to focus on specific parts of the input data, weighing their importance when computing the output. It can be described as a function that maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values, with weights assigned based on the compatibility between the query and the corresponding keys.\n",
            "\n",
            "thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: What is Scaled Dot-Product Attention?\n",
            "Chatbot: Scaled Dot-Product Attention is a type of attention mechanism that computes the dot products of the query with all keys, divides each by √dk, and applies a softmax function to obtain the weights on the values. It is a particular attention mechanism used in the Transformer model, which is designed to counteract the effect of large dot products by scaling them by 1/√dk. This helps to prevent the softmax function from having extremely small gradients when dealing with large values of dk.\n",
            "\n",
            "thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: What are Encoder and Decoder in the Transformer architecture?\n",
            "Chatbot: In the Transformer architecture, the Encoder and Decoder are two main components. \n",
            "\n",
            "The Encoder is composed of a stack of N = 6 identical layers, each with two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network. The Encoder maps an input sequence of symbol representations to a sequence of continuous representations.\n",
            "\n",
            "The Decoder is also composed of a stack of N = 6 identical layers, with three sub-layers in each layer: a multi-head self-attention mechanism over the output of the encoder stack, a multi-head self-attention mechanism over the previous positions in the decoder, and a simple, position-wise fully connected feed-forward network. The Decoder generates an output sequence one element at a time, consuming the previously generated symbols as additional input when generating the next.\n",
            "\n",
            "thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: Describe more about the Transformer architecture.\n",
            "Chatbot: The Transformer architecture is a model that relies entirely on an attention mechanism to draw global dependencies between input and output. It consists of an encoder and a decoder, both of which are composed of a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The Transformer uses multi-head attention in three different ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder. The model also employs residual connections and layer normalization to facilitate the flow of information. The Transformer architecture allows for significantly more parallelization than traditional recurrent neural network (RNN) architectures, making it more efficient and scalable. The model has been shown to achieve state-of-the-art results in translation tasks, such as English-to-German and English-to-French translation, with significantly less training time and computational cost than other models.\n",
            "\n",
            "thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: Why use self-attention in Transformer?\n",
            "Chatbot: Self-attention is used in the Transformer because it has several advantages over traditional recurrent and convolutional layers. It allows the model to jointly attend to information from different representation subspaces at different positions, and it can be parallelized more easily, making it faster for longer sequences. Additionally, self-attention layers have a constant number of operations, whereas recurrent layers require a sequential number of operations, making self-attention more efficient for sequence transduction tasks. \n",
            "\n",
            "thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "def rag_chatbot(document_name):\n",
        "\n",
        "    print(f\"Chatbot initialized.\\nThe chatbot is to help answer anything related to {document_name}.\\n(type 'quit' to stop)\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_question = input(\"You: \")\n",
        "\n",
        "        if user_question.lower() == \"quit\":\n",
        "            break\n",
        "\n",
        "        response = rag_chain.invoke(user_question)\n",
        "        print(\"Chatbot:\", response)\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "# Run the chatbot\n",
        "rag_chatbot(doc_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "state": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}