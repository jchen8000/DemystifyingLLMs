{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jchen8000/DemystifyingLLMs/blob/main/6_Deployment/RAG_LangChain_Groq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNmZBiDG9b_s"
      },
      "source": [
        "# 6. Deployment of LLMs\n",
        "\n",
        "## 6.11 Retrieval Augmented Generation (RAG) Application\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is one of the most powerful applications enabled by LLMs, it's a technique for augmenting LLM knowledge with additional data.\n",
        "\n",
        "Large Language Models (LLMs) are indeed powerful tools for natural language processing (NLP). However, their knowledge is confined to the public data they were trained on. This means they might not be aware of private data, domain specific data or any new information introduced after their training cutoff date.\n",
        "\n",
        "To build AI applications that can reason about such private, domain-specific or up-to-date data, it’s important to augment the model’s knowledge with the specific information it needs. This is what RAG is trying to achieve. RAG involves retrieving the relevant information and incorporating it into the model’s prompt, enabling the LLMs to generate responses based on the most current and specific data available.\n",
        "\n",
        "\n",
        "### **How it works:**\n",
        "\n",
        "1. **Retrieval**:\n",
        "The model analyzes the user's input and retrieves relevant information from a vast knowledge base.\n",
        "This includes documents, conversations, or other textual sources.\n",
        "\n",
        "2. **Augmentation**:\n",
        "The retrieved information is augmented with additional context and knowledge.\n",
        "This includes the user's intent and domain-specific knowledge.\n",
        "\n",
        "3. **Generation**:\n",
        "The augmented information is used to generate a comprehensive and informative response. The model combines the retrieved content with the new context to create a coherent and relevant output.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ **Compatibility Note**\n",
        "\n",
        "This notebook is confirmed working as of **January 2026** in Google Colab. However, LangChain, HuggingFace, as well as other related APIs evolve rapidly, and future updates or model deprecations may require adjustments to this notebook.\n",
        "\n",
        "**This example is for learning and reference purposes and reflects the current best practices, but the APIs used here may change over time.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEW82hQCEpes"
      },
      "source": [
        "### 0. Install the packages and import them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAkhHfkYVZ9L"
      },
      "outputs": [],
      "source": [
        "%pip install -q \\\n",
        "  langchain==1.2.3 \\\n",
        "  langchain-core==1.2.6 \\\n",
        "  langchain-community==0.4.1 \\\n",
        "  langchain-huggingface==1.2.0 \\\n",
        "  langchain-chroma==1.1.0 \\\n",
        "  langchain_groq==1.1.1 \\\n",
        "  faiss-cpu==1.13.2 \\\n",
        "  pypdf===6.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyClqdToU3iH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMP5unl_19FF"
      },
      "source": [
        "### 1. Indexing\n",
        "\n",
        "Indexing is an important process for a RAG application, ensuring the efficient retrieval of the most relevant information.\n",
        "It includes the following steps:\n",
        "\n",
        "* **Load Documents**: The first step is to load the documents or\n",
        "data you want to use. This can be private, domain-specific or up-to-data data including text files, PDFs, web pages, or any other relevant sources.\n",
        "* **Split into Small Chunks**: Once the documents are loaded, they are split into smaller, manageable chunks. This is important because smaller chunks are easier to process and retrieve. The splitting can be based on sentences, paragraphs, or fixed token lengths.\n",
        "* **Embedding**: Each chunk of text is then converted into a vector representation using an embedding model. These embeddings capture the semantic meaning of the text in a numerical format, making it easier for the system to understand and compare different pieces of information.\n",
        "* **Store in a Vector Database**: The final step is to store these vector embeddings in a vector database. This specialized database is optimized for storing and retrieving high-dimensional vectors, allowing for efficient similarity searches.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aufpFjCHICYw"
      },
      "source": [
        "#### Load Document\n",
        "\n",
        "This example is to load a PDF file from a URL. First, we download the PDF to a local file.\n",
        "\n",
        "```PyPDFLoader()``` function provides a way to load and extract text from PDF documents. It comes with ```langchain_community.document_loaders``` libary.\n",
        "\n",
        "```loader.load()``` is to load the PDF document and extract its text content. The result ```document``` is a string or a list of strings representing the text content of the PDF document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mCHUwgXvVMdZ"
      },
      "outputs": [],
      "source": [
        "# Download a PDF file\n",
        "url = \"https://arxiv.org/pdf/1706.03762\"\n",
        "doc_name = \"Attention_Is_All_You_Need.pdf\"\n",
        "\n",
        "response = requests.get(url)\n",
        "with open(doc_name, \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Load the PDF file\n",
        "loader = PyPDFLoader(doc_name)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzOryalPIN_t"
      },
      "source": [
        "#### Split into small trunks\n",
        "\n",
        "```RecursiveCharacterTextSplitter``` is a class that implements a text splitting algorithm, specifically designed for RAG applications. Where ```chunk_size``` specifies the maximum size of each chunk (or split) in characters. ```chunk_overlap``` specifies the amount of overlap between consecutive chunks. In this case, each chunk will overlap with the previous chunk by 200 characters.\n",
        "\n",
        "The purpose of splitting the documents is to create a set of smaller, more manageable pieces of text that can be embedded and indexed in the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9ec1RhR_Zfuc"
      },
      "outputs": [],
      "source": [
        "# Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEBhUJReK81v"
      },
      "source": [
        "Optionally, check how many splits in total, and the content of any given split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ya-EWVYkyU7s"
      },
      "outputs": [],
      "source": [
        "# print('splits:', len(splits))\n",
        "# print('splits[20] metadata:', splits[20].metadata)\n",
        "# print('splits[20] content:', splits[20].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsAts0RWIrNg"
      },
      "source": [
        "#### Embedding\n",
        "\n",
        "Load an embedding model from Hugging Face Transformers library.\n",
        "\n",
        "```HuggingFaceEmbeddings``` is a class that provides an interface for generating embeddings from a pre-trained language model, which in this example is *bert-base-uncased* model.\n",
        "\n",
        "Reference Section 2.8 and Section 3.2 of the book [***Demystifying Large Language Models***](https://github.com/jchen8000/DemystifyingLLMs/) for more details about embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "96ffb7929faa4713af982321138a75db",
            "699af55dcce34baca2fa475bcc9407a9",
            "290c30f64e4d4f37aedb91e280ed924d",
            "c4539b3a695b408fa49a4c5982f04e9e",
            "baba604ad67c4612be224e23e36a0a28",
            "a13a35a1b0a742c6a1d689a9b248cd68",
            "814c61d2ef4e4df08f0d159e24905099",
            "7170780e5fc740b4b4359107017de1a8",
            "490ed9e7bf92406a80213ec78f26624e",
            "c26124c96db340e5813d2d4692c11fe4",
            "0b7e037a643f4cd4b0492a4bd5acef8a",
            "6f38c8be19b74df9bfc07a0e5e11a50d",
            "4bc893280a514184b1dcadf0ce5f014d",
            "cf8bf5cfccd147b5bfb9c5535ad1e0b5",
            "d11c97b83e7d45c593bba2cc79a782af",
            "01715332bf6c44b39d3ce4ac978bc55f",
            "a52cc173754242ae8fe24a14e6f0b5ea",
            "428efde25f34442e9cbe65417c43821b",
            "de08646d80dd4e159f83b2060cb62e70",
            "0e899c8f9b1242babd5580c18902a9eb",
            "099bfa5aa9d241b6a656de09f32f4719",
            "6d4095a2237f4ee3a087f7cc83d05ecb",
            "c3c3888fc50f465dbce42e7a2f13883d",
            "3f1cb3c4cd504922b629241adf342865",
            "8c793f32614a45909c347bec57632a37",
            "f81cceeafc534bf29e8289c634388f90",
            "5c03b50650e74135b2b5f8969cfaf559",
            "b2ed7689a6034224ba962661ccc68c38",
            "fa70fbe881484bc7befd61241972b416",
            "d354db4f79b5403c8d190d99ab932a6b",
            "59d878a69d764d848116f00f8ca4cdd0",
            "93dfd57c9ef84a3a9aee5a9ee54bba4f",
            "2fb02c620d1f4329bbd861890f0d8d0e",
            "e430680d41ed4b679f318148fa4fc799",
            "c1fe2eb5714941de9f3adf43e08ff802",
            "763de9ab721e4a92bee0e0f2cf2c8e65",
            "6b077f4a3e97497389c92f92113acb5d",
            "1c5c63a1b37242a29807b2b32ec599aa",
            "7289657f5e804223acd102c043ff12cc",
            "7893b0157b094669a13c337a37090fcf",
            "f51f830cfdc540c6a061679cb8430c28",
            "bc22df4cb71e4f18b114bbae56a0cb40",
            "358147af62b847d6bdbeba7b538f4944",
            "e51ca2652b084cecb8ea5d3840673ab2",
            "05b5eb115fb0475996f38911dc04a26b",
            "e22bff234fbb4a2bbf2da3b153a7acf2",
            "4631cdeef51d4009b352c551845095f5",
            "952382256cda4e2cb71d2968d2b76d6a",
            "b91dde61a8f24af6a88776e41ce21bc8",
            "34946d0545f24744b58482638aebe09a",
            "eeb1f957ac80454da8113cd5942ceefa",
            "5b3b829a72544b36a4e3b5dbbc81138e",
            "ca10609c16aa40139adbcd1562fbc15a",
            "fec64b71bac840d3869e508da5afe2d6",
            "05f4519a2b074c9e8a4a6a342601745e"
          ]
        },
        "id": "azWLgJaXafYm",
        "outputId": "a19721b8-8bc5-4555-b327-6600348086bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96ffb7929faa4713af982321138a75db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f38c8be19b74df9bfc07a0e5e11a50d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3c3888fc50f465dbce42e7a2f13883d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e430680d41ed4b679f318148fa4fc799",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05b5eb115fb0475996f38911dc04a26b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the embedding model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW_iHI6HI6m3"
      },
      "source": [
        "#### Store in a Vector Database\n",
        "\n",
        "This example uses FAISS (Facebook AI Similarity Search) as the vector database, the function ```FAISS.from_documents(splits, embeddings)``` is to create a FAISS index from the splitted texts and their corresponding embeddings. When the below code is executed, FAISS creates an index that maps each splitted text to its corresponding embedding. This index allows for efficient similarity search, clustering, and other operations on the embeddings.\n",
        "\n",
        "Optionally, you can save the vector database into a local folder.\n",
        "\n",
        "Reference Section 6.9 of the book [***Demystifying Large Language Models***](https://github.com/jchen8000/DemystifyingLLMs/) for more details on vector database. And reference https://github.com/facebookresearch/faiss for FAISS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P9z-6iovnJs0"
      },
      "outputs": [],
      "source": [
        "# Create a vector store\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "# Save the documents and embeddings\n",
        "vectorstore.save_local(\"vectorstore.db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GptIwjU1COn",
        "outputId": "6a93ad38-8ce9-479e-d264-deffef233182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216K\tvectorstore.db\n"
          ]
        }
      ],
      "source": [
        "# Check the size of the vectorstore.db\n",
        "!du -sh vectorstore.db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw9FK1PrDc77"
      },
      "source": [
        "### 2. Retrieval and Generation\n",
        "\n",
        "Retrieval and Generation are essential processes in a RAG application, enabling it to deliver more precise and informed responses by leveraging the power of LLMs along with the private, domain-specific, and up-to-date data stored in the vector database.\n",
        "\n",
        "\n",
        "* **Retrieve Related Information**: When a user submits a query, the system first searches the vector database to find the most relevant chunks of information. These chunks are retrieved based on their semantic similarity to the query.\n",
        "* **Augment the Prompt**: The retrieved information is then used to augment the original query. This is often done using a framework like LangChain, which helps in seamlessly integrating the additional context into the prompt. This step ensures that the language model has all the necessary information to generate a more accurate and contextually relevant response.\n",
        "* **Invoke the LLM**: Finally, the augmented prompt is passed to the Large Language Model (LLM). The LLM processes the combined input and generates a response that leverages both the original query and the retrieved information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcVopGD4LDyE"
      },
      "source": [
        "#### Retrieve Related Information\n",
        "\n",
        "```vectorstore.as_retriever()``` function is used for retrieving relevant information from a vector database, based on their semantic similarity.\n",
        "\n",
        "```format_docs(docs)``` function is defined for formatting the retrieved information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fZU4SutVj1fA"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 8})\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrIqkp0ilHy9"
      },
      "source": [
        "Optionally, you can issue a user query and call ```retriever.invoke(user_query)``` function to check what information is retrived from the ```retriever```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B8UqloIdDXHD"
      },
      "outputs": [],
      "source": [
        "# user_query = \"What is Attention\"\n",
        "# retrieved_docs = retriever.invoke(user_query)\n",
        "# print(len(retrieved_docs))\n",
        "# print(format_docs(retrieved_docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwnq_S4BL6f6"
      },
      "source": [
        "#### Augment the Prompt\n",
        "\n",
        "Create a custom prompt template using ```PromptTemplate.from_template(template)``` function to format the context and user query. The prompt explicitly tells the LLM to answer the question based on the provided context.\n",
        "\n",
        "Then build a model with LangChain that includes:\n",
        "* LLM (Groq with model=\"llama3-70b-8192\" in this example)\n",
        "* Prompt template\n",
        "* Retrived context from the vector database\n",
        "* User's query\n",
        "\n",
        "The code will output a concise and informative answer to the user's query, based on the provided context and the language model's understanding of the conversation.\n",
        "\n",
        "Please note, an API key from Groq Cloud is required for this example. The API key ***GROQ_API_KEY*** is stored in the Secret of Google Colab. Please see other code examples in this chapter for how to do it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr9OKqjiIGd4"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "GROP_MODEL = \"llama-3.3-70b-versatile\"\n",
        "llm = ChatGroq(groq_api_key=userdata.get('GROQ_API_KEY'), model=GROP_MODEL)\n",
        "\n",
        "template = \\\n",
        "\"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "custom_rag_prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlrhu7IexqN3"
      },
      "source": [
        "#### Invoke the LLM\n",
        "\n",
        "Build a simple conversational chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zgjU08G11tC",
        "outputId": "5c9a7d0d-6bbb-4619-da74-7b029364c23f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: What is Transformer?\n",
            "Chatbot: The Transformer is a model architecture that eschews recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. It is a neural network architecture that allows for significantly more parallelization and has achieved state-of-the-art results in translation quality. Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: What is Attention?\n",
            "Chatbot: According to the provided context, attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. It's sometimes called intra-attention.\n",
            "\n",
            "Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: What is Scaled Dot-Product Attention?\n",
            "Chatbot: I don't know. The text does not explicitly define what Scaled Dot-Product Attention is. It only mentions that Noam proposed scaled dot-product attention, but it does not provide a definition or explanation of what it is. Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: What are Encoder and Decoder?\n",
            "Chatbot: According to the context, the Encoder and Decoder are components of the Transformer model architecture.\n",
            "\n",
            "The Encoder is composed of a stack of N=6 identical layers, each layer having two sub-layers: a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network.\n",
            "\n",
            "The Decoder is also composed of a stack of N=6 identical layers, with three sub-layers in each layer: a multi-head self-attention mechanism, a simple, position-wise fully connected feed-forward network, and a third sub-layer that performs multi-head attention over the output of the encoder stack.\n",
            "\n",
            "Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: Describe more about the Transformer.\n",
            "Chatbot: The Transformer is a neural sequence transduction model that relies entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. It has an encoder-decoder structure, where the encoder maps an input sequence of symbol representations to a sequence of continuous representations, and the decoder generates an output sequence based on those representations. The model uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. It employs a residual connection around each sub-layer, followed by layer normalization. The model has been shown to generalize well to other tasks, such as English constituency parsing, and achieves a new state of the art on WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.\n",
            "\n",
            "Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: Why use self-attention?\n",
            "Chatbot: According to the text, self-attention is used because it reduces the total computational complexity per layer and allows for more parallelization, as measured by the minimum number of sequential operations required. Additionally, self-attention reduces the path length between long-range dependencies in the network, making it easier to learn long-range dependencies. Thanks for asking!\n",
            "\n",
            "\n",
            "\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    user_question = input(\"You: \")\n",
        "\n",
        "    if user_question.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    response = rag_chain.invoke(user_question)\n",
        "    print(\"Chatbot:\", response)\n",
        "    print(\"\\n\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNh+2X4E6Dm3ZPz3HlDkrP1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "state": {}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
